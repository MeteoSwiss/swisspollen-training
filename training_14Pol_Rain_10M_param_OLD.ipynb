{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5658a91-89ba-4951-b762-aa07f8c4631b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"Swisens_logo.png\" width=\"240\" height=\"240\" align=\"left\"/>\n",
    "<div style=\"text-align: right\">\n",
    "    SwisensDataAnalyzer Introduction\n",
    "    <br>Machine Learning Model Training\n",
    "    <br>Author: <a href=\"mailto:yanick.zeder@swisens.ch\">Yanick Zeder</a>\n",
    "    <br> Copyright 2021, Swisens AG\n",
    "    <br> <a href=\"mailto:yanick.zeder@swisens.ch\"> Support </a>\n",
    "    <br><br>\n",
    "    <b>Adapted and modified by MeteoSwiss.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e131a44-c57c-47d0-914c-fc5b84acaf65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training ConvNets\n",
    "\n",
    "## Known bugs\n",
    "- Swisens' data loading code sometimes crashes with error \"np.stack ValueError: need at least one array to stack\". Seems to happen only when few datasets are loaded (e.g. when loading only the spores collection).\n",
    "- EfficientNet suffers from a bug which makes it unable to be saved in TF2.10 ([source](https://discuss.tensorflow.org/t/using-efficientnetb0-and-save-model-will-result-unable-to-serialize-2-0896919-2-1128857-2-1081853-to-json-unrecognized-type-class-tensorflow-python-framework-ops-eagertensor/12518https://discuss.tensorflow.org/t/using-efficientnetb0-and-save-model-will-result-unable-to-serialize-2-0896919-2-1128857-2-1081853-to-json-unrecognized-type-class-tensorflow-python-framework-ops-eagertensor/12518))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f8217-c285-4703-98bb-1efacfc042c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93182008-bce1-4315-a513-4502d2cd59d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:06.613140Z",
     "iopub.status.busy": "2023-02-13T07:31:06.612351Z",
     "iopub.status.idle": "2023-02-13T07:31:06.691458Z",
     "shell.execute_reply": "2023-02-13T07:31:06.690318Z",
     "shell.execute_reply.started": "2023-02-13T07:31:06.613093Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\n",
    "    \"/tf/tmp/poleno-ml\"\n",
    ")\n",
    "sys.path.append(\n",
    "    \"/tf/tmp/poleno-db-interface/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95cc27d3-074f-429e-895b-74c2bdbadd08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:06.693645Z",
     "iopub.status.busy": "2023-02-13T07:31:06.693287Z",
     "iopub.status.idle": "2023-02-13T07:31:11.177074Z",
     "shell.execute_reply": "2023-02-13T07:31:11.175473Z",
     "shell.execute_reply.started": "2023-02-13T07:31:06.693610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /tf/tmp/poleno-ml\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: poleno-ml\n",
      "  Building wheel for poleno-ml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for poleno-ml: filename=poleno_ml-0.1.0-py3-none-any.whl size=16657 sha256=e2142a8f67f0b6d3990b3f903742ccbfb4aead9f364b2489ed778d480eccbf24\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/27/94/c36c0ca182dfe6d14b2ad2190409db7ec462f251c1019d9266\n",
      "Successfully built poleno-ml\n",
      "Installing collected packages: poleno-ml\n",
      "  Attempting uninstall: poleno-ml\n",
      "    Found existing installation: poleno-ml 0.1.0\n",
      "    Uninstalling poleno-ml-0.1.0:\n",
      "      Successfully uninstalled poleno-ml-0.1.0\n",
      "Successfully installed poleno-ml-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# run this if you made changes to the poleno-ml code \n",
    "# NB: Those changes must have been made to the /tf/tmp/poleno-ml repository to have an effect on this notebook's code.\n",
    "# NB: However, changes made to the tmp repository are temporary and will be rolled back when Docker VM will be shutdown.\n",
    "#     If you want to make them permanent, dupplicate them to /tf/home/dependencies/poleno-ml.\n",
    "!pip install /tf/tmp/poleno-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99b176c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:11.179984Z",
     "iopub.status.busy": "2023-02-13T07:31:11.179684Z",
     "iopub.status.idle": "2023-02-13T07:31:11.506687Z",
     "shell.execute_reply": "2023-02-13T07:31:11.505373Z",
     "shell.execute_reply.started": "2023-02-13T07:31:11.179955Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Failed for PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "Failed for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Success for PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Import all other necessary modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import datetime\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import poleno_db_interface.database.model.data_explorer_model as dem\n",
    "import poleno_db_interface.database.model.poleno_data_model as pdm\n",
    "from poleno_ml.database.query_interface_ml import QueryInterfaceML, DatasetPipeline\n",
    "import random\n",
    "import sklearn.metrics\n",
    "from sqlalchemy import func\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras as keras\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "from uuid import UUID\n",
    "import uuid\n",
    "\n",
    "# allow memory growth\n",
    "for dev in tf.config.list_physical_devices():\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(\n",
    "            dev, True\n",
    "        )\n",
    "        print(f\"Success for {dev}\")\n",
    "    except:\n",
    "        print(f\"Failed for {dev}\")\n",
    "\n",
    "# specifies which PhysicalDevice objects are visible to the runtime. TF will only allocate memory and place operations on visible physical devices\n",
    "gpu0 = tf.config.list_physical_devices('GPU')[0] # use GPU n\n",
    "tf.config.set_visible_devices(gpu0, 'GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpu0, \n",
    "    #[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=25_000)] # set max GPU memory usage\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=27_000)]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6187ed8f-3a3d-40d2-81db-596724043f06",
   "metadata": {},
   "source": [
    "# Using this strategy will place any variables created in its scope on the specified device. \n",
    "# Input distributed through this strategy will be prefetched to the specified device. \n",
    "# Moreover, any functions called via strategy.run will also be placed on the specified device as well.\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368125b3-7b29-4203-87e3-2790c3a44665",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Establishing a database connection <a class=\"anchor\" id=\"query\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c2ff41d-1093-439c-897a-bd48a3d42399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:11.509145Z",
     "iopub.status.busy": "2023-02-13T07:31:11.508688Z",
     "iopub.status.idle": "2023-02-13T07:31:12.101903Z",
     "shell.execute_reply": "2023-02-13T07:31:12.100623Z",
     "shell.execute_reply.started": "2023-02-13T07:31:11.509113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import myloginpath\n",
    "db_config = myloginpath.parse('client', path='/tf/.mylogin.cnf')\n",
    "\n",
    "# Conect to the database and create an interface instance\n",
    "query_interface_ml = QueryInterfaceML(**db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558f7f7-6e73-4eba-bde1-eab47bf0022e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the datasets and parameters to use <a class=\"anchor\" id=\"datasets\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74a808a8-8a88-4523-b88c-4abb9fcb86d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:12.104400Z",
     "iopub.status.busy": "2023-02-13T07:31:12.104089Z",
     "iopub.status.idle": "2023-02-13T07:31:12.172137Z",
     "shell.execute_reply": "2023-02-13T07:31:12.171161Z",
     "shell.execute_reply.started": "2023-02-13T07:31:12.104371Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A dictionary that has the form of { <collection>: { <dataset-id>: <class-label> } }\n",
    "# Collection name can be choosen freely and is intended to enable grouping of datasets into logical\n",
    "# units independnet on class. Typically, this can be used to seperate datasets form different systems\n",
    "# but can also be used to separate different years, sample source or any other propertie. Later in the\n",
    "# notebook, you will then be able to create test and validation sets with different collections as\n",
    "# source.\n",
    "DATASET_DEFINITIONS = {\n",
    "    \n",
    "    \"raw-pollens\": {\n",
    "        \"11ea8493-7107-8db4-9bf7-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea847a-f995-790c-830f-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea8475-957e-347c-985a-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea8897-f50e-66a2-9876-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8632-18ed-7210-985a-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8632-1eb2-2452-bc84-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8f77-4ee3-aef4-b330-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8f6d-3e75-9fe6-b46e-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8f6d-1562-211a-8192-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8f6c-b78c-d076-a542-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8498-b729-d4e6-bc84-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea8498-b083-cb92-a1a5-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea8498-afa9-cec4-a877-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea8fa9-6c12-723a-b3dd-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea8fa8-fafa-aeb4-ac46-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea8fa8-d163-dce2-b1cb-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea8636-313b-a6e4-a69e-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea8635-ef91-6ab2-a877-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea8635-eb18-6ee0-9876-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea857e-7bc5-60a0-842e-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea857b-3d52-9034-830f-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea857b-150e-c372-bc84-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8af3-c533-f39e-8b25-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea8af1-91fc-9a46-8b25-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea8af0-83dc-6d66-b06c-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea863d-acf6-0ade-985a-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea863c-2449-be52-8814-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea8b83-25c9-8194-90d1-ae7b87f820b4\": \"Platanus\",\n",
    "        \"11ea8881-3721-9aa8-a907-ae7b87f820b4\": \"Platanus\",\n",
    "        \"11ea990f-ee01-8334-b3dd-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea990c-b2bc-fe96-b46e-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11eb5fd9-961a-313e-ac56-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11eb5fd9-dd36-0a20-88f3-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ebe542-660e-0206-80be-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11eb5fc3-03fa-6da2-8b42-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ebe540-187e-9a0c-b0e2-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea8893-edfb-ca84-a877-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea84a0-e89b-43b8-a69e-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea84a0-a2f0-ab8c-a877-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea863e-1fea-0f7c-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea863e-1b86-8226-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea863d-f388-a038-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea8477-cede-e7dc-897d-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea8477-b584-b690-830f-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea8494-33a5-2e4e-bc84-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea849c-df8f-d95e-897d-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea849c-db7b-2170-8b0f-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea849a-0e25-4018-8814-ae7b87f820b4\": \"Ulmus\",\n",
    "    },\n",
    "    \n",
    "    \"old-pollens\": {\n",
    "        \"11ea5df1-de4e-68a2-bdea-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea5dec-aad2-40f2-adc5-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea5dea-76dd-45e6-9881-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea8318-8f19-8414-8f2c-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea831d-c087-d5fa-8016-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8319-0f9b-2f84-8f2c-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea74ee-1ae2-3f42-bdc8-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea74ef-1f33-22e0-b530-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea74ef-7256-f794-8624-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea74ef-cf63-afc2-bdc8-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea5e04-fc94-364e-81ab-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea5e04-ae5b-1ace-9881-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea5e00-93c5-6f88-81ab-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea74e8-0f90-f08a-9ea9-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea74ea-1a2b-69ec-9846-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea74ea-93f3-4862-bc81-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea831e-9169-688c-9d84-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea831e-5779-4480-a7e8-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea831e-0698-1618-8016-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea8314-3a9a-8644-8fc4-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8313-8fef-2358-8016-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8313-1742-26b2-9d84-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8af1-16b3-afbe-b419-ae7b87f820b4\": \"Pinaceae\", # Picea\n",
    "        \"11ea8af1-8668-a68e-b06c-ae7b87f820b4\": \"Pinaceae\", # Picea\n",
    "        \"11ea8af0-7bba-7a4c-9b82-ae7b87f820b4\": \"Pinaceae\", # Picea\n",
    "        \"11ea863b-0dbc-a204-8b0f-ae7b87f820b4\": \"Pinaceae\", # Pinus\n",
    "        \"11ea863c-0128-16ee-a69e-ae7b87f820b4\": \"Pinaceae\", # Pinus\n",
    "        \"11ea831f-8774-33e2-b44c-ae7b87f820b4\": \"Plantanus\",\n",
    "        \"11ea831f-3746-b084-a59a-ae7b87f820b4\": \"Plantanus\",\n",
    "        \"11ea990d-99b7-329e-b330-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea990c-a115-87c6-b46e-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea9a91-09a7-100e-86f2-ae7b87f820b4\": \"Poaceae\", # Cynosurus\n",
    "        \"11ea9a73-0d84-81ca-b3dd-ae7b87f820b4\": \"Poaceae\", # Cynosurus\n",
    "        \"11ebde5f-23d5-5e8c-8d93-ae7b87f820b4\": \"Poaceae\", # Dactylis\n",
    "        \"11ea9911-17a1-65b4-89a8-ae7b87f820b4\": \"Poaceae\", # Dactylis\n",
    "        \"11ebde60-40fb-4aa2-a536-ae7b87f820b4\": \"Poaceae\", # Trisetum\n",
    "        \"11ea830f-df57-8bdc-91a3-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea74e3-b2e6-df38-b530-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea74e4-5089-a266-8624-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea863a-bdae-8df4-a69e-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea863a-2252-7b54-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea8639-acca-ee52-aa3e-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea5deb-b5f0-b38e-9881-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea5deb-6fa4-5f7a-bd51-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea5df3-6b53-0252-adc5-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea74d0-dd4c-4d34-9ea9-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea74cf-fc64-b108-8624-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea5ef1-f146-eabe-ab02-ae7b87f820b4\": \"Ulmus\",\n",
    "    },\n",
    "\n",
    "    #14 classes\n",
    "    \"new-pollens\": { # same datasets as in \"old-pollens\" but cleaned using a different strategy\n",
    "        \"11ed3b18-1ba8-ee6a-a8d4-496190c661df\": \"Alnus\",\n",
    "        \"11ed3821-9ab6-8dba-a8d4-496190c661df\": \"Alnus\",\n",
    "        \"11ed382b-542a-0dea-a8d4-496190c661df\": \"Alnus\",\n",
    "        \"11ed3832-b506-668e-a8d4-496190c661df\": \"Betula\",\n",
    "        \"11ed3a58-07b4-deb6-a8d4-496190c661df\": \"Betula\",\n",
    "        \"11ed38e1-b2f4-f996-a8d4-496190c661df\": \"Betula\",\n",
    "        \"11ed5ea4-96ed-bdf8-acbd-a95f70cb44b0\": \"Carpinus\",\n",
    "        \"11ed5f80-5dcc-9c24-acbd-a95f70cb44b0\": \"Carpinus\",\n",
    "        \"11ed3a6b-5612-556c-a8d4-496190c661df\": \"Carpinus\",\n",
    "        \"11ed3982-4837-efd6-a8d4-496190c661df\": \"Carpinus\",\n",
    "        \"11ed38e0-6bee-47ec-a8d4-496190c661df\": \"Corylus\",\n",
    "        \"11ed3a6f-f2ce-f140-a8d4-496190c661df\": \"Corylus\",\n",
    "        \"11ed431c-682d-5986-a8d4-496190c661df\": \"Corylus\",\n",
    "        \"11ed38ea-0973-6246-a8d4-496190c661df\": \"Cupressus\",\n",
    "        \"11ed3a79-1b42-92c2-a8d4-496190c661df\": \"Cupressus\",\n",
    "        \"11ed431e-c1e0-39ce-a8d4-496190c661df\": \"Cupressus\",\n",
    "        \"11ed3a7c-3218-626c-a8d4-496190c661df\": \"Fagus\",\n",
    "        \"11ed5f45-7fe1-e688-acbd-a95f70cb44b0\": \"Fagus\",\n",
    "        \"11ed5f7b-c645-a4b2-acbd-a95f70cb44b0\": \"Fagus\",\n",
    "        \"11ed3b22-8577-b13c-a8d4-496190c661df\": \"Fraxinus\",\n",
    "        \"11ed5f4a-52c3-34a4-acbd-a95f70cb44b0\": \"Fraxinus\",\n",
    "        \"11ed5f7e-c6f1-7f46-acbd-a95f70cb44b0\": \"Fraxinus\",\n",
    "        \"11ed3d7f-b12e-ad76-a8d4-496190c661df\": \"Pinaceae\", # Picea\n",
    "        \"11ed5f4d-49d0-5748-acbd-a95f70cb44b0\": \"Pinaceae\", # Picea\n",
    "        \"11ed5f82-2199-44da-acbd-a95f70cb44b0\": \"Pinaceae\", # Picea\n",
    "        \"11ed3d9b-7037-b6fc-a8d4-496190c661df\": \"Pinaceae\", # Pinus\n",
    "        \"11ed5494-ad56-7e64-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed5535-c71d-065a-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed55ce-5aaf-a2ae-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed55d0-cec8-bdae-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed55d5-a393-64c2-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed5907-4d09-f0b4-acbd-a95f70cb44b0\": \"Plantanus\",\n",
    "        \"11ed6006-a52a-2448-acbd-a95f70cb44b0\": \"Plantanus\",\n",
    "        \"11ed43d2-4043-b620-a8d4-496190c661df\": \"Poaceae\",\n",
    "        \"11ed6009-3e1d-3184-acbd-a95f70cb44b0\": \"Poaceae\",\n",
    "        \"11ed6030-3f50-655e-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6035-488e-4564-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6037-d194-cad4-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6039-e772-8574-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed591a-064b-e868-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed591c-0732-bf5c-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed5922-9f70-fc56-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed5925-0949-93b6-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed5926-9845-1346-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6032-1974-ee34-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed6036-c877-51c0-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed603b-8b16-bcf8-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed6038-9eb8-c42a-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed6010-ad76-a770-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed4487-97ec-87e8-a8d4-496190c661df\": \"Poaceae\", # Trisetum\n",
    "        \"11ed5928-4b40-315a-acbd-a95f70cb44b0\": \"Populus\",\n",
    "        \"11ed6012-cffe-1c4a-acbd-a95f70cb44b0\": \"Populus\",\n",
    "        \"11ed6041-5f45-1876-acbd-a95f70cb44b0\": \"Populus\",\n",
    "        \"11ed5930-a266-322e-acbd-a95f70cb44b0\": \"Quercus\",\n",
    "        \"11ed601d-4f24-ca46-acbd-a95f70cb44b0\": \"Quercus\",\n",
    "        \"11ed6045-38bd-92ec-acbd-a95f70cb44b0\": \"Quercus\",\n",
    "        \"11ed59c8-6d52-bbaa-acbd-a95f70cb44b0\": \"Taxus\",\n",
    "        \"11ed602a-3932-49e0-acbd-a95f70cb44b0\": \"Taxus\",\n",
    "        \"11ed604c-7275-f1b2-acbd-a95f70cb44b0\": \"Taxus\",\n",
    "        \"11ed59ca-df2c-2188-acbd-a95f70cb44b0\": \"Ulmus\",\n",
    "        \"11ed602d-6874-225c-acbd-a95f70cb44b0\": \"Ulmus\",\n",
    "        \"11ed6048-9fb9-4b28-acbd-a95f70cb44b0\": \"Ulmus\",\n",
    "    },\n",
    "    \n",
    "    \"trash\": {\n",
    "        \"11eda79a-f000-b3a2-85ae-ae7b87f820b4\": \"Trash\",\n",
    "    },\n",
    "    \n",
    "    \"other\": {\n",
    "        \"11ed65b6-6a22-1968-b56b-ae7b87f820b4\": \"Artemisia\",\n",
    "        \"11ed463e-f09f-6456-b550-ae7b87f820b4\": \"Cedrus\",\n",
    "        \"11ec6179-fde0-042c-adac-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ec617a-fb8b-e3f2-80fb-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ec5821-b371-c3dc-8359-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ec5832-4fee-03b4-8561-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ebe542-f782-c172-bf10-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebeabd-e224-d5c4-8b63-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebedec-0da5-47ac-8066-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebee15-1fea-4c68-9cd6-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \n",
    "    },\n",
    "    \n",
    "    \"waterdroplets\":{\n",
    "        \"11ebe542-f782-c172-bf10-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebeabd-e224-d5c4-8b63-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebedec-0da5-47ac-8066-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebee15-1fea-4c68-9cd6-ae7b87f820b4\": \"Waterdroplets\",\n",
    "    },\n",
    "    \n",
    "    \"spores\": {\n",
    "        \"11ebf9db-f2e9-98cc-bc67-ae7b87f820b4\": \"Alternaria Solani\",\n",
    "        \"11ec01b9-d571-ea8e-b7e1-ae7b87f820b4\": \"Fusarium Graminearum\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94d844c1-6637-4471-a841-e44a04568a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:12.173603Z",
     "iopub.status.busy": "2023-02-13T07:31:12.173308Z",
     "iopub.status.idle": "2023-02-13T07:31:12.220247Z",
     "shell.execute_reply": "2023-02-13T07:31:12.219408Z",
     "shell.execute_reply.started": "2023-02-13T07:31:12.173575Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = '14Pol_Rain_10M_param_OLD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caa66bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:12.221694Z",
     "iopub.status.busy": "2023-02-13T07:31:12.221410Z",
     "iopub.status.idle": "2023-02-13T07:31:12.269889Z",
     "shell.execute_reply": "2023-02-13T07:31:12.269021Z",
     "shell.execute_reply.started": "2023-02-13T07:31:12.221667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8 # the smaller, the more difficult the training becomes but it also generally means better generalization\n",
    "epochs = 256 # max number of epochs (early stopping automatically interrupts the training if validation loss stops improving)\n",
    "img_shape = (200,200,1)\n",
    "model_features = [\n",
    "     'rec0', 'rec1'  \n",
    "] # 'rec0' and 'rec1' for holo images\n",
    "data_filters = [\n",
    "    #'blur', # remove blurry events\n",
    "    #'crop', # remove cropped events\n",
    "]\n",
    "data_maps = [\n",
    "    #'process_waves', # remove \"waves\" from all events\n",
    "    #'holo_aug', # image augmentation\n",
    "] # if you change the training data, you might want to apply the same transformations to the polenos as pre-processing steps\n",
    "caching = True # whether or not cache the datasets locally for better performance\n",
    "\n",
    "collections_train = [\n",
    "    #\"raw-pollens\",\n",
    "    \"old-pollens\",\n",
    "    #\"new-pollens\",\n",
    "    #\"other\",\n",
    "    #\"spores\",\n",
    "    \"waterdroplets\",\n",
    "]\n",
    "\n",
    "# These two values only apply when collections_val is empty\n",
    "train_part = 0.7\n",
    "test_part = 0.3\n",
    "\n",
    "# Leave empty if you'd like to train and eval on the same collections\n",
    "collections_val = []\n",
    "\n",
    "classes = set(cls \n",
    "              for collection in collections_train + collections_val \n",
    "              for cls in DATASET_DEFINITIONS[collection].values()) # not need to touch this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de1ee9b7-b414-4d0e-a210-60c876339357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:12.271506Z",
     "iopub.status.busy": "2023-02-13T07:31:12.271215Z",
     "iopub.status.idle": "2023-02-13T07:31:12.322602Z",
     "shell.execute_reply": "2023-02-13T07:31:12.321673Z",
     "shell.execute_reply.started": "2023-02-13T07:31:12.271478Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paths are created\n",
    "model_path = 'models'\n",
    "os.makedirs(os.path.join(model_path, model_name, \"training\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(model_path, model_name, \"model\"), exist_ok=True)\n",
    "ds_train_cache_path = os.path.join(model_path, model_name, 'training', f'train_cache_{\"_\".join(collections_train)}{\"_\" + \"_\".join(data_filters + data_maps) if len(data_filters + data_maps) > 0 else \"\"}')\n",
    "if len(collections_val) == 0:\n",
    "    cache_name = f'test_cache_{\"_\".join(collections_train)}'\n",
    "else:\n",
    "    cache_name = f'test_cache_{\"_\".join(collections_val)}'\n",
    "ds_val_cache_path = os.path.join(model_path, model_name, 'training', f'{cache_name}{\"_\" + \"_\".join(data_filters + data_maps) if len(data_filters + data_maps) > 0 else \"\"}')\n",
    "model_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_file_path = os.path.join(model_path, model_name, 'training', 'checkpoints', model_timestamp)\n",
    "model_file_path = os.path.join(model_path, model_name, 'model')\n",
    "model_info_file_path = os.path.join(model_path, model_name, 'model', 'model_info.json')\n",
    "logdir = os.path.join(model_path, model_name, 'training', 'logs', model_timestamp)\n",
    "os.makedirs(logdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5958366-6d3f-42a5-b2db-4b7d8d6eb7af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:12.326434Z",
     "iopub.status.busy": "2023-02-13T07:31:12.326137Z",
     "iopub.status.idle": "2023-02-13T07:31:12.373079Z",
     "shell.execute_reply": "2023-02-13T07:31:12.371955Z",
     "shell.execute_reply.started": "2023-02-13T07:31:12.326406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model's info to save on disk\n",
    "model_info = {\n",
    "    'model_name': model_name,\n",
    "    'model_timestamp': model_timestamp,\n",
    "    'batch_size': batch_size,\n",
    "    'model_features': model_features,\n",
    "    'data_filters': data_filters,\n",
    "    'data_maps': data_maps,\n",
    "    'collections_train': collections_train,\n",
    "    'train_part': train_part,\n",
    "    'test_part': test_part,\n",
    "    'collections_val': collections_val,\n",
    "    'DATASET_DEFINITIONS': DATASET_DEFINITIONS,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d074e54-8821-4cd9-99cc-32e73df216f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computed Values\n",
    "Here we define the class weights for unbalanced datasets, the class counts, the class labels and we compute the model name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693dd81-ded3-469e-ac9f-245e5f8525b9",
   "metadata": {},
   "source": [
    "First, we define some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e24e7b74-6ca9-437d-bbdb-8b26ca07d9a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:12.375661Z",
     "iopub.status.busy": "2023-02-13T07:31:12.375367Z",
     "iopub.status.idle": "2023-02-13T07:31:12.425681Z",
     "shell.execute_reply": "2023-02-13T07:31:12.424717Z",
     "shell.execute_reply.started": "2023-02-13T07:31:12.375633Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_mapping(ds_map: dict, collections: List[str], classes: List[str]):\n",
    "    \"\"\"This method filters a dataset definition for specific systems and class labels\n",
    "    and returns a flat dictionary with { <dataset-id>: <class-label> } \"\"\"\n",
    "    \n",
    "    ret = {}\n",
    "    for c in collections:\n",
    "        ret.update(\n",
    "            { key: value for key, value in ds_map[c].items() if value in classes}\n",
    "        )\n",
    "    return ret\n",
    "\n",
    "def get_dataset_sizes(ds_map_flat: dict):\n",
    "    \"\"\"Return a dict with <dataset-id>: <class-size>\"\"\"\n",
    "    \n",
    "    dataset_sizes = {}\n",
    "    for k, v in dataset_map.items():\n",
    "        result = query_interface_ml.session.query(\n",
    "            func.count(dem.EventsInEventDataset.event_id)\n",
    "        ).filter(\n",
    "            dem.EventsInEventDataset.dataset_id==uuid.UUID(k).bytes\n",
    "        ).all()\n",
    "        dataset_sizes[k] = result[0][0]\n",
    "    return dataset_sizes\n",
    "    \n",
    "def get_class_sizes(ds_map_flat: dict, dataset_sizes: dict):\n",
    "    \"\"\"Return a dict with <class-name>: <class-size>\"\"\"\n",
    "    \n",
    "    class_sizes = {}\n",
    "    for k, v in dataset_map.items():\n",
    "        size = dataset_sizes[k]\n",
    "        if v not in class_sizes: class_sizes[v] = 0\n",
    "        class_sizes[v] += size\n",
    "    return class_sizes\n",
    "\n",
    "def get_sorted_class_list(ds_map_flat: dict):\n",
    "    return sorted(list(set(ds_map_flat.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86d5aa-7390-4f1e-9efc-0a215708a2b7",
   "metadata": {},
   "source": [
    "... and then we can compute the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "599bc8ca-b765-4cd7-a647-76fe212866ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:12.427253Z",
     "iopub.status.busy": "2023-02-13T07:31:12.426961Z",
     "iopub.status.idle": "2023-02-13T07:31:19.633016Z",
     "shell.execute_reply": "2023-02-13T07:31:19.631702Z",
     "shell.execute_reply.started": "2023-02-13T07:31:12.427224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the dataset collection you need for the training\n",
    "dataset_map = get_dataset_mapping(\n",
    "    DATASET_DEFINITIONS,\n",
    "    collections=collections_train,\n",
    "    classes=classes\n",
    ")\n",
    "assert len(dataset_map) > 0\n",
    "\n",
    "dataset_sizes = get_dataset_sizes(dataset_map)\n",
    "class_sizes = get_class_sizes(dataset_map, dataset_sizes)\n",
    "classes = get_sorted_class_list(dataset_map)\n",
    "num_classes = len(classes)\n",
    "\n",
    "n_samples = sum(dataset_sizes.values())\n",
    "class_counts = [class_sizes[d] for d in classes]\n",
    "class_weights = n_samples / np.array(class_counts)\n",
    "model_info['classes'] = classes\n",
    "model_info['class_weights'] = list(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd092e8-80f5-4e86-aef2-d62a8fb8dc89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup the dataset pipeline <a class=\"anchor\" id=\"pipeline\"></a>\n",
    "This step builds the dataset pipeline used later to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01de7a-758e-4c86-acc3-1ab74d3e4fbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17c78e34-d206-46fe-83d6-04e3443ed6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:31:19.635142Z",
     "iopub.status.busy": "2023-02-13T07:31:19.634757Z",
     "iopub.status.idle": "2023-02-13T07:34:31.810000Z",
     "shell.execute_reply": "2023-02-13T07:34:31.808811Z",
     "shell.execute_reply.started": "2023-02-13T07:31:19.635113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(collections_val) == 0:\n",
    "\n",
    "    dataset_train, dataset_val = query_interface_ml.prepare_tf_dataset_from_poleno_datasets(\n",
    "        dataset_list=list(dataset_map.keys()),\n",
    "        batch_size=batch_size,\n",
    "        model_features=model_features,\n",
    "        labels=classes,\n",
    "        dataset_label_mapping=dataset_map,\n",
    "        split=(train_part, test_part)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    dataset_map_val = get_dataset_mapping(\n",
    "        DATASET_DEFINITIONS,\n",
    "        collections=collections_val,\n",
    "        classes=classes\n",
    "    )\n",
    "    \n",
    "    dataset_train = query_interface_ml.prepare_tf_dataset_from_poleno_datasets(\n",
    "        dataset_list=list(dataset_map.keys()),\n",
    "        batch_size=batch_size,\n",
    "        model_features=model_features,\n",
    "        labels=classes,\n",
    "        dataset_label_mapping=dataset_map,\n",
    "    )\n",
    "    \n",
    "    dataset_val = query_interface_ml.prepare_tf_dataset_from_poleno_datasets(\n",
    "        dataset_list=list(dataset_map_val.keys()),\n",
    "        batch_size=batch_size,\n",
    "        model_features=model_features,\n",
    "        labels=classes,\n",
    "        dataset_label_mapping=dataset_map_val,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f649c94-4c3c-46bf-a48f-a3fe81fdb695",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define then apply filters and maps (e.g. data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f733876-810b-4c5d-a0e8-f1d750c5523c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:34:31.811550Z",
     "iopub.status.busy": "2023-02-13T07:34:31.811311Z",
     "iopub.status.idle": "2023-02-13T07:34:31.856572Z",
     "shell.execute_reply": "2023-02-13T07:34:31.855632Z",
     "shell.execute_reply.started": "2023-02-13T07:34:31.811528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define data filters\n",
    "# filters out images where particles are cropped\n",
    "def filter_crop(rec0: tf.Tensor, rec1: tf.Tensor, T: float = .0001, BT: float = .85):\n",
    "    border = [0,rec0.shape[0]-1]\n",
    "    mask = np.array([\n",
    "        [1. if i in border or j in border else 0. for j in range(rec0.shape[1])]\n",
    "        for i in range(rec0.shape[0])\n",
    "    ]).reshape(*rec0.shape)\n",
    "    apply_filter_crop_ = lambda x: ((x.numpy()<BT)*mask).sum() / mask.sum() > T # return True if particle is cropped\n",
    "    return not apply_filter_crop_(rec0) and not apply_filter_crop_(rec1)\n",
    "apply_filter_crop = lambda ids, features, targets: tf.py_function( #py_function to work in eager mode (dataset operations are in graph mode by default)\n",
    "    filter_crop, [features['rec0'], features['rec1']], Tout=tf.bool\n",
    ")\n",
    "\n",
    "# filters out images where particles are blurry\n",
    "def filter_blur(rec0: tf.Tensor, rec1: tf.Tensor, T: float = .0014):\n",
    "    apply_filter_blur_ = lambda x: cv2.Laplacian(x.numpy(), cv2.CV_32F).var() < T # return True if image is blurred\n",
    "    return not apply_filter_blur_(rec0) and not apply_filter_blur_(rec1)\n",
    "apply_filter_blur = lambda ids, features, targets: tf.py_function( #py_function to work in eager mode (dataset operations are in graph mode by default)\n",
    "    filter_blur, [features['rec0'], features['rec1']], Tout=tf.bool\n",
    ")\n",
    "\n",
    "def filter_test(rec0: tf.Tensor, rec1: tf.Tensor):\n",
    "    apply_filter_test_ = lambda x: True\n",
    "    return not apply_filter_test_(rec0) and not apply_filter_test_(rec1)\n",
    "apply_filter_test = lambda ids, features, targets: tf.py_function( #py_function to work in eager mode (dataset operations are in graph mode by default)\n",
    "    filter_test, [features['rec0'], features['rec1']], Tout=tf.bool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "435b97ab-6010-4b06-8cbe-a134436069bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:34:31.858442Z",
     "iopub.status.busy": "2023-02-13T07:34:31.857992Z",
     "iopub.status.idle": "2023-02-13T07:34:31.924795Z",
     "shell.execute_reply": "2023-02-13T07:34:31.923649Z",
     "shell.execute_reply.started": "2023-02-13T07:34:31.858402Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define data maps:\n",
    "# removes \"waves\" around the particles\n",
    "def rmv_waves(rec: tf.Tensor):\n",
    "    img = (rec.numpy()*255).astype(np.uint8)\n",
    "    img = img.reshape(*img.shape[:-1])\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    _, mask = cv2.threshold(blurred, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    cleaned = img * ~(mask).astype(bool)\n",
    "    return cleaned\n",
    "\n",
    "# performs random image augmentation\n",
    "upper = 120 * (math.pi/180.0) # degrees -> radian\n",
    "lower = -120 * (math.pi/180.0)\n",
    "def rand_degree():\n",
    "    return random.uniform(lower, upper)\n",
    "def augment_using_ops(img: tf.Tensor):\n",
    "    try: img = tf.image.random_flip_left_right(img)\n",
    "    except: print(\"flip hori\")\n",
    "    try: img = tf.image.random_flip_up_down(img)\n",
    "    except: print(\"flip vert\")\n",
    "    try: img = tf.image.random_brightness(img, 0.1)\n",
    "    except: print(\"brightness\")\n",
    "    try: img = tf.image.random_contrast(img, 0.7, 1.3)\n",
    "    except: print(\"contrast\")\n",
    "    try: img = tfa.image.rotate(img, rand_degree(), fill_mode='nearest') # fill_mode='constant', fill_value=1.\n",
    "    except: print(\"rotate\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "841c3ed4-3568-40c7-bca6-f21277e319d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:34:31.926301Z",
     "iopub.status.busy": "2023-02-13T07:34:31.925948Z",
     "iopub.status.idle": "2023-02-13T07:34:31.981597Z",
     "shell.execute_reply": "2023-02-13T07:34:31.980606Z",
     "shell.execute_reply.started": "2023-02-13T07:34:31.926271Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_custom_holo_map(func, *args):\n",
    "    func_ = lambda x: tf.reshape(\n",
    "        tf.convert_to_tensor(\n",
    "            tf.py_function(func, [x], Tout=[tf.float32]), \n",
    "            dtype=tf.float32),\n",
    "        img_shape)\n",
    "    args[1]['rec0'] = func_(args[1]['rec0'])\n",
    "    args[1]['rec1'] = func_(args[1]['rec1'])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef37950a-3d1a-45dd-9164-6b9c00c8fcbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:34:31.983503Z",
     "iopub.status.busy": "2023-02-13T07:34:31.983162Z",
     "iopub.status.idle": "2023-02-13T07:34:32.058175Z",
     "shell.execute_reply": "2023-02-13T07:34:32.057149Z",
     "shell.execute_reply.started": "2023-02-13T07:34:31.983467Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply data filters and data maps\n",
    "dataset_train.tf_dataset = dataset_train.tf_dataset.unbatch()\n",
    "dataset_val.tf_dataset = dataset_val.tf_dataset.unbatch()\n",
    "\n",
    "if 'blur' in data_filters:\n",
    "    dataset_train.tf_dataset = dataset_train.tf_dataset.filter(apply_filter_blur)\n",
    "    dataset_val.tf_dataset = dataset_val.tf_dataset.filter(apply_filter_blur)\n",
    "if 'crop' in data_filters:\n",
    "    dataset_train.tf_dataset = dataset_train.tf_dataset.filter(apply_filter_crop)\n",
    "    dataset_val.tf_dataset = dataset_val.tf_dataset.filter(apply_filter_crop)\n",
    "if 'process_waves' in data_maps:\n",
    "    dataset_train.tf_dataset = dataset_train.tf_dataset.map(lambda *args: apply_custom_holo_map(rmv_waves, *args), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    dataset_val.tf_dataset = dataset_val.tf_dataset.map(lambda *args: apply_custom_holo_map(rmv_waves, *args), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "if 'holo_aug' in data_maps:\n",
    "    augmented_train = dataset_train.tf_dataset.map(lambda *args: apply_custom_holo_map(augment_using_ops, *args), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    dataset_train.tf_dataset = dataset_train.tf_dataset.concatenate(augmented_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6babf2-bc38-4f81-aec7-ee91c98008e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cache, shuffle, batch, and prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17a53246-2f33-4e79-bcb5-50d4c394d73e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T07:34:32.059607Z",
     "iopub.status.busy": "2023-02-13T07:34:32.059317Z",
     "iopub.status.idle": "2023-02-13T09:10:48.570554Z",
     "shell.execute_reply": "2023-02-13T09:10:48.569127Z",
     "shell.execute_reply.started": "2023-02-13T07:34:32.059579Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION: Remember to remove the cache file if you make changes to the dataset! Otherwise, the changes will not be reflected into the dataset and the trainingwill run on the old data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e22795c3e8460b94965d01bdab80f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing cache:   0%|          | 0/163760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching is done.\n"
     ]
    }
   ],
   "source": [
    "# Cache the pipeline to a file in order to make subsequent training passes much faster\n",
    "if caching:\n",
    "    # Cache the pipeline to a file in order to make subsequent training passes much faster\n",
    "    dataset_train.enable_cache(ds_train_cache_path, prepare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1287110-dcec-4fec-9b19-a656338f472e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:10:48.572460Z",
     "iopub.status.busy": "2023-02-13T09:10:48.572133Z",
     "iopub.status.idle": "2023-02-13T09:46:40.282028Z",
     "shell.execute_reply": "2023-02-13T09:46:40.280691Z",
     "shell.execute_reply.started": "2023-02-13T09:10:48.572426Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION: Remember to remove the cache file if you make changes to the dataset! Otherwise, the changes will not be reflected into the dataset and the trainingwill run on the old data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc03756e71147e4956e44470f86c471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing cache:   0%|          | 0/70184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching is done.\n"
     ]
    }
   ],
   "source": [
    "if caching:\n",
    "    dataset_val.enable_cache(ds_val_cache_path, prepare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41bd43a7-b369-4466-9660-94a082c03a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:40.284032Z",
     "iopub.status.busy": "2023-02-13T09:46:40.283715Z",
     "iopub.status.idle": "2023-02-13T09:46:40.355003Z",
     "shell.execute_reply": "2023-02-13T09:46:40.353576Z",
     "shell.execute_reply.started": "2023-02-13T09:46:40.284000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train.tf_dataset = dataset_train.tf_dataset.shuffle(batch_size*100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "dataset_val.tf_dataset = dataset_val.tf_dataset.shuffle(batch_size*100, reshuffle_each_iteration=False).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9067a508-85a4-46c8-939e-b8349e3340bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-05T17:22:39.621895Z",
     "iopub.status.busy": "2022-12-05T17:22:39.621481Z",
     "iopub.status.idle": "2022-12-05T17:23:39.165824Z",
     "shell.execute_reply": "2022-12-05T17:23:39.164012Z",
     "shell.execute_reply.started": "2022-12-05T17:22:39.621855Z"
    },
    "tags": []
   },
   "source": [
    "# run this to get the real number of events in the datasets\n",
    "# note: this can take a long time and fill your ram if the dataset's too large\n",
    "'%i train, %i validation' % (len(list(dataset_train.tf_dataset.as_numpy_iterator())), len(list(dataset_val.tf_dataset.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84f9c892-3a63-4c8a-aa67-156c997fb9ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:40.356621Z",
     "iopub.status.busy": "2023-02-13T09:46:40.356332Z",
     "iopub.status.idle": "2023-02-13T09:46:40.406395Z",
     "shell.execute_reply": "2023-02-13T09:46:40.405217Z",
     "shell.execute_reply.started": "2023-02-13T09:46:40.356593Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset lengths: (train: 163760, val: 70184)\n",
      "number of classes: (train: 15, val: 15)\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset lengths: (train: {dataset_train.dataset_length}, val: {dataset_val.dataset_length})\")\n",
    "print(f\"number of classes: (train: {dataset_train.num_classes}, val: {dataset_val.num_classes})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f7a1d-3969-4a0b-b1a1-0138599b9fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build the model <a class=\"anchor\" id=\"model\"></a>\n",
    "Now we can build the model we want to train. You can use any model as long as it has:\n",
    "- two inputs with the shape 200x200x1 with the names 'rec0' and 'rec1'\n",
    "- an output layer with the name 'target' and a dimension of #classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12da4f8-cee1-4d36-b39c-ef94b440cded",
   "metadata": {},
   "source": [
    "#### Operationnal model\n",
    "\n",
    "Use the model defined by Nina and Benoit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6281ad4-a03b-44da-8049-6afcec3caf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:40.407943Z",
     "iopub.status.busy": "2023-02-13T09:46:40.407658Z",
     "iopub.status.idle": "2023-02-13T09:46:40.764736Z",
     "shell.execute_reply": "2023-02-13T09:46:40.763509Z",
     "shell.execute_reply.started": "2023-02-13T09:46:40.407916Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = []\n",
    "paths = []\n",
    "\n",
    "if \"rec0\" in model_features and \"rec1\" in model_features:\n",
    "    input0 = keras.layers.Input(shape=[200,200,1], name=\"rec0\")\n",
    "    inputs.append(input0)\n",
    "    input1 = keras.layers.Input(shape=[200,200,1], name=\"rec1\")\n",
    "    inputs.append(input1)\n",
    "\n",
    "    ###                                   -> VVVV <- Change here to B0 - B7 if needed.\n",
    "    path0 = keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(input0)\n",
    "    path0 = keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path0)\n",
    "    path0 = keras.layers.Dropout(0.2)(path0)\n",
    "    path0 = keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path0)\n",
    "    path0 = keras.layers.Dropout(0.2)(path0)\n",
    "    path0 = keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path0)\n",
    "    path0 = keras.layers.Dropout(0.2)(path0)\n",
    "    path0 = keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path0)\n",
    "    path0 = keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path0)\n",
    "    path0 = keras.layers.Dropout(0.2)(path0)\n",
    "\n",
    "    path1 = keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(input1)\n",
    "    path1 = keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path1)\n",
    "    path1 = keras.layers.Dropout(0.2)(path1)\n",
    "    path1 = keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path1)\n",
    "    path1 = keras.layers.Dropout(0.2)(path1)\n",
    "    path1 = keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path1)\n",
    "    path1 = keras.layers.Dropout(0.2)(path1)\n",
    "    path1 = keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "    path1 = keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path1)\n",
    "    path1 = keras.layers.Dropout(0.2)(path1)\n",
    "\n",
    "    ### \n",
    "   \n",
    "    holo_path_ = keras.layers.Concatenate()([path0, path1])\n",
    "    holo_path_ = keras.layers.Flatten()(holo_path_)\n",
    "    \n",
    "    paths.append(holo_path_)\n",
    "\n",
    "if \"fl_spectra\" in model_features:\n",
    "    input_fl = keras.layers.Input(shape=[13], name=\"fl_spectra\")\n",
    "    fl_path = input_fl * 255\n",
    "    fl_path = keras.layers.Dense(255)(fl_path)\n",
    "    inputs.append(input_fl)\n",
    "    paths.append(fl_path)\n",
    "\n",
    "if len(paths) > 1:\n",
    "    path_ = keras.layers.Concatenate()(paths)\n",
    "else:\n",
    "    path_ = paths[0]\n",
    "\n",
    "path_ = tf.keras.layers.Dense(64)(path_)\n",
    "path_ = tf.keras.layers.Dropout(0.2)(path_)\n",
    "\n",
    "#path_ = keras.layers.Dropout(.2)(path_)\n",
    "outputs = keras.layers.Dense(\n",
    "    num_classes,\n",
    "    activation=\"softmax\",\n",
    "    name=\"target\"\n",
    ")(path_)\n",
    "#outputs = keras.layers.Softmax(name=\"target\")(outputs)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[outputs])\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651abab8-ad5c-4f5a-bc37-25820cc06450",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pre-trained EffNet\n",
    "This uses transfer learning in order to get faster results. You can choose between 8 models B0 - B7, each a little larger than the previous. The way this is set up is that we remove the final layer of the pretrained network and add our own output layer corresponding to our classes.\n",
    "\n",
    "Note that during our tests, B0 was sufficient for most applications. With larger models you increase the tendency to overfit the data. If you have many classes and large datasets, you can consider increasing the model size."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5de44d6-8270-4d4c-84dd-398ff431bf97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T15:04:41.073289Z",
     "iopub.status.busy": "2022-12-20T15:04:41.073004Z",
     "iopub.status.idle": "2022-12-20T15:04:41.120865Z",
     "shell.execute_reply": "2022-12-20T15:04:41.119909Z",
     "shell.execute_reply.started": "2022-12-20T15:04:41.073261Z"
    },
    "tags": []
   },
   "source": [
    "# WARNING: THIS IS NOT A GOOD IDEA TO DO IF YOU DOWNLOAD DATA FROM UNKNOW OR UNVERIFIED SOURCES !!!\n",
    "# THIS COULD INDUCE HUGE SECURITY RISKS.\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "raw",
   "id": "611fc803-0db7-4f2f-9dc8-ecb15955f876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T15:04:41.122307Z",
     "iopub.status.busy": "2022-12-20T15:04:41.122039Z",
     "iopub.status.idle": "2022-12-20T15:04:45.057342Z",
     "shell.execute_reply": "2022-12-20T15:04:45.055724Z",
     "shell.execute_reply.started": "2022-12-20T15:04:41.122280Z"
    },
    "tags": []
   },
   "source": [
    "inputs = []\n",
    "paths = []\n",
    "\n",
    "if \"rec0\" in model_features and \"rec1\" in model_features:\n",
    "    input0 = keras.layers.Input(shape=[200,200,1], name=\"rec0\")\n",
    "    inputs.append(input0)\n",
    "    input1 = keras.layers.Input(shape=[200,200,1], name=\"rec1\")\n",
    "    inputs.append(input1)\n",
    "\n",
    "    input0_reshape = keras.layers.Concatenate()([input0,input0,input0])\n",
    "    input1_reshape = keras.layers.Concatenate()([input1,input1,input1])\n",
    "    input0_reshape = input0_reshape * 255 # effnet expects [0, 255] data range\n",
    "    input1_reshape = input1_reshape * 255 # effnet expects [0, 255] data range\n",
    "\n",
    "    ###                                   -> VVVV <- Change here to B0 - B7 if needed.\n",
    "    effnetB0 = keras.applications.EfficientNetB0(input_shape=(200,200,3), \n",
    "                                                 drop_connect_rate=0.4, # extra regularization in finetuning, but does not affect loaded weights\n",
    "                                                 include_top=False, weights='imagenet')\n",
    "    effnetB0.trainable = False\n",
    "\n",
    "    path0 = effnetB0(input0_reshape)\n",
    "    path1 = effnetB0(input1_reshape)\n",
    "\n",
    "    holo_path_ = keras.layers.Concatenate()([path0, path1])\n",
    "    holo_path_ = keras.layers.Flatten()(holo_path_)\n",
    "    \n",
    "    paths.append(holo_path_)\n",
    "\n",
    "if \"fl_spectra\" in model_features:\n",
    "    input_fl = keras.layers.Input(shape=[13], name=\"fl_spectra\")\n",
    "    fl_path = input_fl * 255\n",
    "    fl_path = keras.layers.Dense(255)(fl_path)\n",
    "    inputs.append(input_fl)\n",
    "    paths.append(fl_path)\n",
    "\n",
    "if len(paths) > 1:\n",
    "    path_ = keras.layers.Concatenate()(paths)\n",
    "else:\n",
    "    path_ = paths[0]\n",
    "\n",
    "path_ = keras.layers.Dropout(.4)(path_)\n",
    "outputs = keras.layers.Dense(\n",
    "    num_classes,\n",
    "    activation=\"sigmoid\",\n",
    "    name=\"target\"\n",
    ")(path_)\n",
    "#outputs = keras.layers.Softmax(name=\"target\")(outputs)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[outputs])\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfe34ce8-136a-4cb7-95c9-99252c6eeddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:40.766379Z",
     "iopub.status.busy": "2023-02-13T09:46:40.766077Z",
     "iopub.status.idle": "2023-02-13T09:46:40.914175Z",
     "shell.execute_reply": "2023-02-13T09:46:40.913113Z",
     "shell.execute_reply.started": "2023-02-13T09:46:40.766350Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " rec0 (InputLayer)              [(None, 200, 200, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rec1 (InputLayer)              [(None, 200, 200, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 200, 200, 64  1664        ['rec0[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 200, 200, 64  1664        ['rec1[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 200, 200, 64  102464      ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 200, 200, 64  102464      ['conv2d_10[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 100, 100, 64  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 100, 100, 64  0          ['conv2d_11[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100, 100, 64  0           ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 100, 100, 64  0           ['max_pooling2d_4[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 100, 100, 64  36928       ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 100, 100, 64  36928       ['dropout_4[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 100, 100, 64  36928       ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 100, 100, 64  36928       ['conv2d_12[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 50, 50, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 50, 50, 64)  0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 50, 64)   0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 50, 50, 64)   0           ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 50, 50, 128)  73856       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 50, 50, 128)  73856       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 50, 50, 128)  147584      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 50, 50, 128)  147584      ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 50, 50, 128)  147584      ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 50, 50, 128)  147584      ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 25, 25, 128)  0          ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 25, 25, 128)  0          ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 25, 25, 128)  0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 25, 25, 128)  0           ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 25, 25, 256)  295168      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 25, 25, 256)  295168      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 25, 25, 256)  590080      ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 25, 25, 256)  590080      ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 25, 25, 256)  590080      ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 25, 25, 256)  590080      ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 13, 13, 256)  0          ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 13, 13, 256)  0          ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 13, 13, 256)  0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 13, 13, 256)  0           ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 13, 13, 512)  0           ['dropout_3[0][0]',              \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 86528)        0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           5537856     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " target (Dense)                 (None, 15)           975         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,583,503\n",
      "Trainable params: 9,583,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0409e34-5c77-44a4-b899-c9f696aa197e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile the model\n",
    "By compiling we are adding the optimizer function and the loss function which drives the actual training. Usually, this can be left as is.\n",
    "\n",
    "If you notice that the model trains very slowly -> increase learning_rate <br>\n",
    "If the loss is fluctuating or rising -> decrease learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd3269b7-ffba-40d5-8201-176537e10f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:40.915613Z",
     "iopub.status.busy": "2023-02-13T09:46:40.915326Z",
     "iopub.status.idle": "2023-02-13T09:46:40.970894Z",
     "shell.execute_reply": "2023-02-13T09:46:40.969610Z",
     "shell.execute_reply.started": "2023-02-13T09:46:40.915586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To compile the model, we need a special loss funtion that allows for class weights. The details are not that important. Relavent to know\n",
    "# is that this allows for inbalanced data to be trained correctly. For example, if you have two times more of class 1 than in class 2, then\n",
    "# you would like the model to ignore this difference and act as if the two sets are identical in size.\n",
    "\n",
    "class WeightedSCCE(tf.keras.losses.Loss):\n",
    "    \"\"\"Custom SparseCategoricalCrossentropy loss class that supports class weights.\"\"\"\n",
    "    def __init__(self, class_weight, from_logits=False, name='weighted_scce'):\n",
    "        if class_weight is None or all(v == 1. for v in class_weight):\n",
    "            self.class_weight = None\n",
    "        else:\n",
    "            self.class_weight = tf.convert_to_tensor(class_weight,\n",
    "                dtype=tf.float32)\n",
    "        self.reduction = keras.losses.Reduction.NONE\n",
    "        self.unreduced_scce = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=from_logits, name=name,\n",
    "            reduction=self.reduction)\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        loss = self.unreduced_scce(y_true, y_pred, sample_weight)\n",
    "        if self.class_weight is not None:\n",
    "            weight_mask = tf.gather(self.class_weight, y_true)\n",
    "            loss = tf.math.multiply(loss, weight_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cf0661e-3240-4e4f-8e24-aeac32f9caef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:40.972278Z",
     "iopub.status.busy": "2023-02-13T09:46:40.971991Z",
     "iopub.status.idle": "2023-02-13T09:46:41.034883Z",
     "shell.execute_reply": "2023-02-13T09:46:41.033793Z",
     "shell.execute_reply.started": "2023-02-13T09:46:40.972251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally we compile the ml model\n",
    "learning_rate = 0.000_005\n",
    "model.compile(\n",
    "    # Optimizer, that handles the weight adjustment while training\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),  \n",
    "    # Loss function to minimize\n",
    "    loss=WeightedSCCE(class_weights),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa6896-5250-443c-8897-d31bd389484e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up TensorBoard\n",
    "You can either access TensorBoard from the widgets below or by forwarding port 6006 to your localhost and access from your local browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af32a826-c31a-410b-aa73-64a058ed6172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:41.036288Z",
     "iopub.status.busy": "2023-02-13T09:46:41.036000Z",
     "iopub.status.idle": "2023-02-13T09:46:41.085981Z",
     "shell.execute_reply": "2023-02-13T09:46:41.084880Z",
     "shell.execute_reply.started": "2023-02-13T09:46:41.036261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "# https://stackoverflow.com/questions/40106949/unable-to-open-tensorboard-in-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7477158-7931-474d-b817-a232409f17de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:41.087443Z",
     "iopub.status.busy": "2023-02-13T09:46:41.087159Z",
     "iopub.status.idle": "2023-02-13T09:46:47.106712Z",
     "shell.execute_reply": "2023-02-13T09:46:47.105085Z",
     "shell.execute_reply.started": "2023-02-13T09:46:41.087416Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-61faf13cf6bbdeab\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-61faf13cf6bbdeab\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir} --bind_all --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4d0bd-3d54-4cc7-93e4-f18281ccfe35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81f2b7f7-0aa6-477f-8ad0-cfc20410f712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:47.112192Z",
     "iopub.status.busy": "2023-02-13T09:46:47.111890Z",
     "iopub.status.idle": "2023-02-13T09:46:47.177166Z",
     "shell.execute_reply": "2023-02-13T09:46:47.176110Z",
     "shell.execute_reply.started": "2023-02-13T09:46:47.112164Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confusion matrix plotting at each epoch in TensorBoard\n",
    "# source: https://towardsdatascience.com/exploring-confusion-matrix-evolution-on-tensorboard-e66b39f4ac12 by Surhrut Ashtikar, last visited on 28.11.2022\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "       class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    ax.set(xticks=tick_marks,\n",
    "           yticks=tick_marks,\n",
    "           xticklabels=class_names, \n",
    "           yticklabels=class_names,\n",
    "           title='Confusion matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return fig\n",
    "    \n",
    "def plot_to_image(figure):\n",
    "    \"\"\"\n",
    "    Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\n",
    "    \"\"\"\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    \n",
    "    # Use plt.savefig to save the plot to a PNG in memory.\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Use tf.image.decode_png to convert the PNG buffer\n",
    "    # to a TF image. Make sure you use 4 channels.\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Use tf.expand_dims to add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    \n",
    "    # Use the model to predict the values from the test_images.\n",
    "    val_pred_raw = model.predict(dataset_val.get_data_pipeline().prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    val_pred = np.argmax(val_pred_raw, axis=1)\n",
    "    \n",
    "    # Calculate the confusion matrix using sklearn.metrics\n",
    "    cm = sklearn.metrics.confusion_matrix(val_labels, val_pred)\n",
    "    \n",
    "    figure = plot_confusion_matrix(cm, class_names=classes)\n",
    "    cm_image = plot_to_image(figure)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cff1eeec-0a4d-4787-a5e3-1e235f529e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:46:47.178962Z",
     "iopub.status.busy": "2023-02-13T09:46:47.178674Z",
     "iopub.status.idle": "2023-02-13T09:47:13.191058Z",
     "shell.execute_reply": "2023-02-13T09:47:13.189936Z",
     "shell.execute_reply.started": "2023-02-13T09:46:47.178935Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a737ac806d42bf9eede728bfa48fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_labels = []  # store true labels\n",
    "for _, label_batch in tqdm(dataset_val.get_data_pipeline()): # iterate over the validation dataset\n",
    "    val_labels.extend(label_batch[\"target\"].numpy()) # append true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "213cae0f-976a-449c-a5e8-098d4490a618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:47:13.192828Z",
     "iopub.status.busy": "2023-02-13T09:47:13.192515Z",
     "iopub.status.idle": "2023-02-13T09:47:13.249357Z",
     "shell.execute_reply": "2023-02-13T09:47:13.248105Z",
     "shell.execute_reply.started": "2023-02-13T09:47:13.192798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init tensorflow callbacks\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n",
    "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_file_path, save_weights_only=True, save_best_only=True, monitor='val_loss', mode='min')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78d7cbeb-d5a5-4c83-8b01-754d6c3f0da9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T09:47:13.250839Z",
     "iopub.status.busy": "2023-02-13T09:47:13.250542Z",
     "iopub.status.idle": "2023-02-13T14:22:27.423121Z",
     "shell.execute_reply": "2023-02-13T14:22:27.422400Z",
     "shell.execute_reply.started": "2023-02-13T09:47:13.250811Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "8773/8773 [==============================] - 66s 7ms/step\n",
      "20470/20470 [==============================] - 692s 33ms/step - loss: 34.3619 - sparse_categorical_accuracy: 0.2593 - val_loss: 30.1131 - val_sparse_categorical_accuracy: 0.3763\n",
      "Epoch 2/256\n",
      "8773/8773 [==============================] - 66s 7ms/step\n",
      "20470/20470 [==============================] - 683s 33ms/step - loss: 26.8457 - sparse_categorical_accuracy: 0.4487 - val_loss: 26.4459 - val_sparse_categorical_accuracy: 0.4833\n",
      "Epoch 3/256\n",
      "8773/8773 [==============================] - 68s 8ms/step\n",
      "20470/20470 [==============================] - 678s 33ms/step - loss: 23.2036 - sparse_categorical_accuracy: 0.5253 - val_loss: 24.3397 - val_sparse_categorical_accuracy: 0.5112\n",
      "Epoch 4/256\n",
      "8773/8773 [==============================] - 66s 7ms/step\n",
      "20470/20470 [==============================] - 672s 33ms/step - loss: 21.0723 - sparse_categorical_accuracy: 0.5671 - val_loss: 24.6022 - val_sparse_categorical_accuracy: 0.5091\n",
      "Epoch 5/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 670s 33ms/step - loss: 19.5150 - sparse_categorical_accuracy: 0.5970 - val_loss: 24.5885 - val_sparse_categorical_accuracy: 0.5016\n",
      "Epoch 6/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 671s 33ms/step - loss: 18.2820 - sparse_categorical_accuracy: 0.6210 - val_loss: 21.7071 - val_sparse_categorical_accuracy: 0.5574\n",
      "Epoch 7/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 671s 33ms/step - loss: 17.2819 - sparse_categorical_accuracy: 0.6420 - val_loss: 18.8629 - val_sparse_categorical_accuracy: 0.6066\n",
      "Epoch 8/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 676s 33ms/step - loss: 16.4448 - sparse_categorical_accuracy: 0.6590 - val_loss: 17.5173 - val_sparse_categorical_accuracy: 0.6401\n",
      "Epoch 9/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 671s 33ms/step - loss: 15.7199 - sparse_categorical_accuracy: 0.6723 - val_loss: 16.8668 - val_sparse_categorical_accuracy: 0.6616\n",
      "Epoch 10/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 671s 33ms/step - loss: 15.0404 - sparse_categorical_accuracy: 0.6879 - val_loss: 18.5680 - val_sparse_categorical_accuracy: 0.6217\n",
      "Epoch 11/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 668s 33ms/step - loss: 14.4618 - sparse_categorical_accuracy: 0.6982 - val_loss: 16.0067 - val_sparse_categorical_accuracy: 0.6782\n",
      "Epoch 12/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 670s 33ms/step - loss: 13.9333 - sparse_categorical_accuracy: 0.7088 - val_loss: 16.4501 - val_sparse_categorical_accuracy: 0.6704\n",
      "Epoch 13/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 686s 33ms/step - loss: 13.4607 - sparse_categorical_accuracy: 0.7188 - val_loss: 15.1211 - val_sparse_categorical_accuracy: 0.6916\n",
      "Epoch 14/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 678s 33ms/step - loss: 12.9382 - sparse_categorical_accuracy: 0.7280 - val_loss: 15.3180 - val_sparse_categorical_accuracy: 0.6934\n",
      "Epoch 15/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 672s 33ms/step - loss: 12.5089 - sparse_categorical_accuracy: 0.7363 - val_loss: 15.3191 - val_sparse_categorical_accuracy: 0.6814\n",
      "Epoch 16/256\n",
      "8773/8773 [==============================] - 119s 14ms/step\n",
      "20470/20470 [==============================] - 1174s 57ms/step - loss: 12.1047 - sparse_categorical_accuracy: 0.7438 - val_loss: 13.8930 - val_sparse_categorical_accuracy: 0.7277\n",
      "Epoch 17/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 1195s 58ms/step - loss: 11.6769 - sparse_categorical_accuracy: 0.7521 - val_loss: 13.6592 - val_sparse_categorical_accuracy: 0.7241\n",
      "Epoch 18/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 674s 33ms/step - loss: 11.2544 - sparse_categorical_accuracy: 0.7613 - val_loss: 13.5775 - val_sparse_categorical_accuracy: 0.7363\n",
      "Epoch 19/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 669s 33ms/step - loss: 10.9235 - sparse_categorical_accuracy: 0.7677 - val_loss: 13.9639 - val_sparse_categorical_accuracy: 0.7316\n",
      "Epoch 20/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 668s 33ms/step - loss: 10.5515 - sparse_categorical_accuracy: 0.7751 - val_loss: 15.2970 - val_sparse_categorical_accuracy: 0.7175\n",
      "Epoch 21/256\n",
      "8773/8773 [==============================] - 65s 7ms/step\n",
      "20470/20470 [==============================] - 673s 33ms/step - loss: 10.1609 - sparse_categorical_accuracy: 0.7829 - val_loss: 13.5878 - val_sparse_categorical_accuracy: 0.7398\n",
      "Epoch 22/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 662s 32ms/step - loss: 9.8143 - sparse_categorical_accuracy: 0.7889 - val_loss: 14.1168 - val_sparse_categorical_accuracy: 0.7207\n",
      "Epoch 23/256\n",
      "8773/8773 [==============================] - 64s 7ms/step\n",
      "20470/20470 [==============================] - 667s 33ms/step - loss: 9.5076 - sparse_categorical_accuracy: 0.7937 - val_loss: 14.0370 - val_sparse_categorical_accuracy: 0.7350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff094371250>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset_train.get_data_pipeline().prefetch(tf.data.AUTOTUNE),\n",
    "    epochs=epochs, \n",
    "    validation_data=dataset_val.get_data_pipeline().prefetch(tf.data.AUTOTUNE),\n",
    "    callbacks=[\n",
    "        early_stopping, \n",
    "        checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        cm_callback\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "958cd4c5-37b3-4ed7-b07a-670d495c32a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T14:22:27.424354Z",
     "iopub.status.busy": "2023-02-13T14:22:27.424149Z",
     "iopub.status.idle": "2023-02-13T14:22:32.723036Z",
     "shell.execute_reply": "2023-02-13T14:22:32.721086Z",
     "shell.execute_reply.started": "2023-02-13T14:22:27.424333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/14Pol_Rain_10M_param_OLD/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/14Pol_Rain_10M_param_OLD/model/assets\n"
     ]
    }
   ],
   "source": [
    "# reload last checkpoint's weights those are the ones to export\n",
    "model.load_weights(checkpoint_file_path)\n",
    "# save the best model\n",
    "model.save(model_file_path)\n",
    "# save the model's essential info\n",
    "with open(model_info_file_path, 'w') as f:\n",
    "    f.write(json.dumps(model_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9856a-173a-4860-a405-eedd6d3ce387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
