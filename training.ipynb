{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5658a91-89ba-4951-b762-aa07f8c4631b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"Swisens_logo.png\" width=\"240\" height=\"240\" align=\"left\"/>\n",
    "<div style=\"text-align: right\">\n",
    "    SwisensDataAnalyzer Introduction\n",
    "    <br>Machine Learning Model Training\n",
    "    <br>Author: <a href=\"mailto:yanick.zeder@swisens.ch\">Yanick Zeder</a>\n",
    "    <br> Copyright 2021, Swisens AG\n",
    "    <br> <a href=\"mailto:yanick.zeder@swisens.ch\"> Support </a>\n",
    "</div>\n",
    "Adapted and modified by MeteoSwiss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e131a44-c57c-47d0-914c-fc5b84acaf65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SwisensDataAnalyzer introduction - train ml models\n",
    "This notebook is a short introduction on how to train a machine learning model on the data stored in a Swisens database.\n",
    "\n",
    "## Prerequisites\n",
    "The following modules must be installed in your python environment:\n",
    "- `poleno_db_interface`\n",
    "- `poleno_ml`\n",
    "\n",
    "## Required knowledge\n",
    "- How to use the SwisensDataExplorer (especially how to create and work with datasets)\n",
    "\n",
    "## Content\n",
    "\n",
    "### Topics\n",
    "This notebook takes you along the ride to train a ml model on datasets on the Swisens Database. First, we will see how we can set up a training pipeline. Afterwards, the pipeline is used to train a basic ML model. Then, we check the model on the test data and evaluate using a confusion matrix. Finally, we apply the model to a time series from a SwisensPoleno to evaluate the real-world performance of the model.\n",
    "\n",
    "### Table of contents\n",
    "1. [Add your DB credentials and connect](#query)\n",
    "2. [Defining the datasets to use](#datasets)\n",
    "3. [Setup the database connection and the dataset pipeline](#pipeline)\n",
    "4. [Build the model](#model)\n",
    "5. [Model evaluation](#evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f8217-c285-4703-98bb-1efacfc042c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93182008-bce1-4315-a513-4502d2cd59d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:12.886101Z",
     "iopub.status.busy": "2022-12-01T14:59:12.885678Z",
     "iopub.status.idle": "2022-12-01T14:59:12.893586Z",
     "shell.execute_reply": "2022-12-01T14:59:12.892157Z",
     "shell.execute_reply.started": "2022-12-01T14:59:12.886008Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\n",
    "    \"/tf/tmp/poleno-ml\"\n",
    ")\n",
    "sys.path.append(\n",
    "    \"/tf/tmp/poleno-db-interface/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b176c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:12.895510Z",
     "iopub.status.busy": "2022-12-01T14:59:12.895142Z",
     "iopub.status.idle": "2022-12-01T14:59:17.037159Z",
     "shell.execute_reply": "2022-12-01T14:59:17.035837Z",
     "shell.execute_reply.started": "2022-12-01T14:59:12.895475Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "Success for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Success for PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "for dev in tf.config.list_physical_devices():\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(\n",
    "            dev, True\n",
    "        )\n",
    "        print(f\"Success for {dev}\")\n",
    "    except:\n",
    "        print(f\"Failed for {dev}\")\n",
    "\n",
    "# specifies which PhysicalDevice objects are visible to the runtime. TF will only allocate memory and place operations on visible physical devices\n",
    "gpu0 = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.set_visible_devices(gpu0, 'GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpu0, \n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=25_000)]\n",
    ") \n",
    "\n",
    "# Import all other necessary modules\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from sqlalchemy import func\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from poleno_ml.database.query_interface_ml import QueryInterfaceML\n",
    "from poleno_ml.database.query_interface_ml import DatasetPipeline\n",
    "import operator\n",
    "import poleno_db_interface.database.model.data_explorer_model as dem\n",
    "import poleno_db_interface.database.model.poleno_data_model as pdm\n",
    "from uuid import UUID\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6187ed8f-3a3d-40d2-81db-596724043f06",
   "metadata": {},
   "source": [
    "# Using this strategy will place any variables created in its scope on the specified device. \n",
    "# Input distributed through this strategy will be prefetched to the specified device. \n",
    "# Moreover, any functions called via strategy.run will also be placed on the specified device as well.\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368125b3-7b29-4203-87e3-2790c3a44665",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Establishing a database connection <a class=\"anchor\" id=\"query\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2ff41d-1093-439c-897a-bd48a3d42399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.038994Z",
     "iopub.status.busy": "2022-12-01T14:59:17.038646Z",
     "iopub.status.idle": "2022-12-01T14:59:17.195372Z",
     "shell.execute_reply": "2022-12-01T14:59:17.193645Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.038959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import myloginpath\n",
    "db_config = myloginpath.parse('client', path='/tf/.mylogin.cnf')\n",
    "#db_config['database'] = 'sensor_data_schema'\n",
    "\n",
    "# Conect to the database and create an interface instance\n",
    "query_interface_ml = QueryInterfaceML(**db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e543b1e-12e1-4720-b790-b7867c9239a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create the report structure\n",
    "\n",
    "In order to get a report at the end of the training, we first initialize the report data structure. This is simply a dictionary that will steadily be filled\n",
    "data that we would like to have in our end report. The syntax is straight forward as you see below. You can also add your own data. You just have to make \n",
    "sure that you also add a placeholder for that data in the report template (TODO: Add link to jump to the report template)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8d6b76-7484-4b80-8553-89fd3763c075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.197329Z",
     "iopub.status.busy": "2022-12-01T14:59:17.196610Z",
     "iopub.status.idle": "2022-12-01T14:59:17.242306Z",
     "shell.execute_reply": "2022-12-01T14:59:17.241430Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.197295Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_markdown_table(header, array):\n",
    "    \"\"\" Create a markdown table from a list. \"\"\"\n",
    "\n",
    "    nl = \"\\n\"\n",
    "\n",
    "    markdown = nl\n",
    "    markdown += f\"| {' | '.join(header)} |\"\n",
    "\n",
    "    markdown += nl\n",
    "    markdown += f\"| {' | '.join(['---']*len(header))} |\"\n",
    "\n",
    "    markdown += nl\n",
    "    for entry in array:\n",
    "        str_array = [str(e) for e in entry]\n",
    "        markdown += f\"| {' | '.join(str_array)} |{nl}\"\n",
    "\n",
    "    return markdown\n",
    "\n",
    "report_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558f7f7-6e73-4eba-bde1-eab47bf0022e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the datasets to use <a class=\"anchor\" id=\"datasets\"></a>\n",
    "    \n",
    "<b>ATTENTION: If you change the values in here, make sure you delete all dataset cache files. These are called 'train_cache*' or 'test_cache*' and are located at the same location as this notebook. If you do not delete these caches, the training will run on the old dataset definition!!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a808a8-8a88-4523-b88c-4abb9fcb86d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.244153Z",
     "iopub.status.busy": "2022-12-01T14:59:17.243614Z",
     "iopub.status.idle": "2022-12-01T14:59:17.301763Z",
     "shell.execute_reply": "2022-12-01T14:59:17.299510Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.244121Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A dictionary that has the form of { <collection>: { <dataset-id>: <class-label> } }\n",
    "# Collection name can be choosen freely and is intended to enable grouping of datasets into logical\n",
    "# units independnet on class. Typically, this can be used to seperate datasets form different systems\n",
    "# but can also be used to separate different years, sample source or any other propertie. Later in the\n",
    "# notebook, you will then be able to create test and validation sets with different collections as\n",
    "# source.\n",
    "DATASET_DEFINITIONS = {\n",
    "    \n",
    "    \"raw-pollens\": {\n",
    "        \"11ea8493-7107-8db4-9bf7-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea847a-f995-790c-830f-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea8475-957e-347c-985a-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea8897-f50e-66a2-9876-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8632-18ed-7210-985a-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8632-1eb2-2452-bc84-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8f77-4ee3-aef4-b330-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8f6d-3e75-9fe6-b46e-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8f6d-1562-211a-8192-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8f6c-b78c-d076-a542-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea8498-b729-d4e6-bc84-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea8498-b083-cb92-a1a5-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea8498-afa9-cec4-a877-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea8fa9-6c12-723a-b3dd-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea8fa8-fafa-aeb4-ac46-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea8fa8-d163-dce2-b1cb-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea8636-313b-a6e4-a69e-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea8635-ef91-6ab2-a877-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea8635-eb18-6ee0-9876-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea857e-7bc5-60a0-842e-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea857b-3d52-9034-830f-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea857b-150e-c372-bc84-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8af3-c533-f39e-8b25-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea8af1-91fc-9a46-8b25-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea8af0-83dc-6d66-b06c-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea863d-acf6-0ade-985a-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea863c-2449-be52-8814-ae7b87f820b4\": \"Pinaceae\",\n",
    "        \"11ea8b83-25c9-8194-90d1-ae7b87f820b4\": \"Platanus\",\n",
    "        \"11ea8881-3721-9aa8-a907-ae7b87f820b4\": \"Platanus\",\n",
    "        \"11ea990f-ee01-8334-b3dd-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea990c-b2bc-fe96-b46e-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11eb5fd9-961a-313e-ac56-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11eb5fd9-dd36-0a20-88f3-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ebe542-660e-0206-80be-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11eb5fc3-03fa-6da2-8b42-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ebe540-187e-9a0c-b0e2-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea8893-edfb-ca84-a877-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea84a0-e89b-43b8-a69e-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea84a0-a2f0-ab8c-a877-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea863e-1fea-0f7c-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea863e-1b86-8226-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea863d-f388-a038-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea8477-cede-e7dc-897d-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea8477-b584-b690-830f-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea8494-33a5-2e4e-bc84-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea849c-df8f-d95e-897d-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea849c-db7b-2170-8b0f-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea849a-0e25-4018-8814-ae7b87f820b4\": \"Ulmus\",\n",
    "    },\n",
    "    \n",
    "    \"old-pollens\": {\n",
    "        \"11ea5df1-de4e-68a2-bdea-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea5dec-aad2-40f2-adc5-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea5dea-76dd-45e6-9881-ae7b87f820b4\": \"Alnus\",\n",
    "        \"11ea8318-8f19-8414-8f2c-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea831d-c087-d5fa-8016-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea8319-0f9b-2f84-8f2c-ae7b87f820b4\": \"Betula\",\n",
    "        \"11ea74ee-1ae2-3f42-bdc8-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea74ef-1f33-22e0-b530-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea74ef-7256-f794-8624-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea74ef-cf63-afc2-bdc8-ae7b87f820b4\": \"Carpinus\",\n",
    "        \"11ea5e04-fc94-364e-81ab-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea5e04-ae5b-1ace-9881-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea5e00-93c5-6f88-81ab-ae7b87f820b4\": \"Corylus\",\n",
    "        \"11ea74e8-0f90-f08a-9ea9-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea74ea-1a2b-69ec-9846-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea74ea-93f3-4862-bc81-ae7b87f820b4\": \"Cupressus\",\n",
    "        \"11ea831e-9169-688c-9d84-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea831e-5779-4480-a7e8-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea831e-0698-1618-8016-ae7b87f820b4\": \"Fagus\",\n",
    "        \"11ea8314-3a9a-8644-8fc4-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8313-8fef-2358-8016-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8313-1742-26b2-9d84-ae7b87f820b4\": \"Fraxinus\",\n",
    "        \"11ea8af1-16b3-afbe-b419-ae7b87f820b4\": \"Pinaceae\", # Picea\n",
    "        \"11ea8af1-8668-a68e-b06c-ae7b87f820b4\": \"Pinaceae\", # Picea\n",
    "        \"11ea8af0-7bba-7a4c-9b82-ae7b87f820b4\": \"Pinaceae\", # Picea\n",
    "        \"11ea863b-0dbc-a204-8b0f-ae7b87f820b4\": \"Pinaceae\", # Pinus\n",
    "        \"11ea863c-0128-16ee-a69e-ae7b87f820b4\": \"Pinaceae\", # Pinus\n",
    "        \"11ea831f-8774-33e2-b44c-ae7b87f820b4\": \"Plantanus\",\n",
    "        \"11ea831f-3746-b084-a59a-ae7b87f820b4\": \"Plantanus\",\n",
    "        \"11ea990d-99b7-329e-b330-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea990c-a115-87c6-b46e-ae7b87f820b4\": \"Poaceae\",\n",
    "        \"11ea9a91-09a7-100e-86f2-ae7b87f820b4\": \"Poaceae\", # Cynosurus\n",
    "        \"11ea9a73-0d84-81ca-b3dd-ae7b87f820b4\": \"Poaceae\", # Cynosurus\n",
    "        \"11ebde5f-23d5-5e8c-8d93-ae7b87f820b4\": \"Poaceae\", # Dactylis\n",
    "        \"11ea9911-17a1-65b4-89a8-ae7b87f820b4\": \"Poaceae\", # Dactylis\n",
    "        \"11ebde60-40fb-4aa2-a536-ae7b87f820b4\": \"Poaceae\", # Trisetum\n",
    "        \"11ea830f-df57-8bdc-91a3-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea74e3-b2e6-df38-b530-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea74e4-5089-a266-8624-ae7b87f820b4\": \"Populus\",\n",
    "        \"11ea863a-bdae-8df4-a69e-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea863a-2252-7b54-a1a5-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea8639-acca-ee52-aa3e-ae7b87f820b4\": \"Quercus\",\n",
    "        \"11ea5deb-b5f0-b38e-9881-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea5deb-6fa4-5f7a-bd51-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea5df3-6b53-0252-adc5-ae7b87f820b4\": \"Taxus\",\n",
    "        \"11ea74d0-dd4c-4d34-9ea9-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea74cf-fc64-b108-8624-ae7b87f820b4\": \"Ulmus\",\n",
    "        \"11ea5ef1-f146-eabe-ab02-ae7b87f820b4\": \"Ulmus\",\n",
    "    },\n",
    "    \n",
    "    \"new-pollens\": { # same datasets as in \"old-pollens\" but cleaned using a different strategy\n",
    "        \"11ed3b18-1ba8-ee6a-a8d4-496190c661df\": \"Alnus\",\n",
    "        \"11ed3821-9ab6-8dba-a8d4-496190c661df\": \"Alnus\",\n",
    "        \"11ed382b-542a-0dea-a8d4-496190c661df\": \"Alnus\",\n",
    "        \"11ed3832-b506-668e-a8d4-496190c661df\": \"Betula\",\n",
    "        \"11ed3a58-07b4-deb6-a8d4-496190c661df\": \"Betula\",\n",
    "        \"11ed38e1-b2f4-f996-a8d4-496190c661df\": \"Betula\",\n",
    "        \"11ed5ea4-96ed-bdf8-acbd-a95f70cb44b0\": \"Carpinus\",\n",
    "        \"11ed5f80-5dcc-9c24-acbd-a95f70cb44b0\": \"Carpinus\",\n",
    "        \"11ed3a6b-5612-556c-a8d4-496190c661df\": \"Carpinus\",\n",
    "        \"11ed3982-4837-efd6-a8d4-496190c661df\": \"Carpinus\",\n",
    "        \"11ed38e0-6bee-47ec-a8d4-496190c661df\": \"Corylus\",\n",
    "        \"11ed3a6f-f2ce-f140-a8d4-496190c661df\": \"Corylus\",\n",
    "        \"11ed431c-682d-5986-a8d4-496190c661df\": \"Corylus\",\n",
    "        \"11ed38ea-0973-6246-a8d4-496190c661df\": \"Cupressus\",\n",
    "        \"11ed3a79-1b42-92c2-a8d4-496190c661df\": \"Cupressus\",\n",
    "        \"11ed431e-c1e0-39ce-a8d4-496190c661df\": \"Cupressus\",\n",
    "        \"11ed3a7c-3218-626c-a8d4-496190c661df\": \"Fagus\",\n",
    "        \"11ed5f45-7fe1-e688-acbd-a95f70cb44b0\": \"Fagus\",\n",
    "        \"11ed5f7b-c645-a4b2-acbd-a95f70cb44b0\": \"Fagus\",\n",
    "        \"11ed3b22-8577-b13c-a8d4-496190c661df\": \"Fraxinus\",\n",
    "        \"11ed5f4a-52c3-34a4-acbd-a95f70cb44b0\": \"Fraxinus\",\n",
    "        \"11ed5f7e-c6f1-7f46-acbd-a95f70cb44b0\": \"Fraxinus\",\n",
    "        \"11ed3d7f-b12e-ad76-a8d4-496190c661df\": \"Pinaceae\", # Picea\n",
    "        \"11ed5f4d-49d0-5748-acbd-a95f70cb44b0\": \"Pinaceae\", # Picea\n",
    "        \"11ed5f82-2199-44da-acbd-a95f70cb44b0\": \"Pinaceae\", # Picea\n",
    "        \"11ed3d9b-7037-b6fc-a8d4-496190c661df\": \"Pinaceae\", # Pinus\n",
    "        \"11ed5494-ad56-7e64-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed5535-c71d-065a-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed55ce-5aaf-a2ae-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed55d0-cec8-bdae-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed55d5-a393-64c2-acbd-a95f70cb44b0\": \"Pinaceae\", # Pinus\n",
    "        \"11ed5907-4d09-f0b4-acbd-a95f70cb44b0\": \"Plantanus\",\n",
    "        \"11ed6006-a52a-2448-acbd-a95f70cb44b0\": \"Plantanus\",\n",
    "        \"11ed43d2-4043-b620-a8d4-496190c661df\": \"Poaceae\",\n",
    "        \"11ed6009-3e1d-3184-acbd-a95f70cb44b0\": \"Poaceae\",\n",
    "        \"11ed6030-3f50-655e-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6035-488e-4564-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6037-d194-cad4-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6039-e772-8574-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed591a-064b-e868-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed591c-0732-bf5c-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed5922-9f70-fc56-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed5925-0949-93b6-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed5926-9845-1346-acbd-a95f70cb44b0\": \"Poaceae\", # Cynosurus\n",
    "        \"11ed6032-1974-ee34-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed6036-c877-51c0-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed603b-8b16-bcf8-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed6038-9eb8-c42a-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed6010-ad76-a770-acbd-a95f70cb44b0\": \"Poaceae\", # Dactylis\n",
    "        \"11ed4487-97ec-87e8-a8d4-496190c661df\": \"Poaceae\", # Trisetum\n",
    "        \"11ed5928-4b40-315a-acbd-a95f70cb44b0\": \"Populus\",\n",
    "        \"11ed6012-cffe-1c4a-acbd-a95f70cb44b0\": \"Populus\",\n",
    "        \"11ed6041-5f45-1876-acbd-a95f70cb44b0\": \"Populus\",\n",
    "        \"11ed5930-a266-322e-acbd-a95f70cb44b0\": \"Quercus\",\n",
    "        \"11ed601d-4f24-ca46-acbd-a95f70cb44b0\": \"Quercus\",\n",
    "        \"11ed6045-38bd-92ec-acbd-a95f70cb44b0\": \"Quercus\",\n",
    "        \"11ed59c8-6d52-bbaa-acbd-a95f70cb44b0\": \"Taxus\",\n",
    "        \"11ed602a-3932-49e0-acbd-a95f70cb44b0\": \"Taxus\",\n",
    "        \"11ed604c-7275-f1b2-acbd-a95f70cb44b0\": \"Taxus\",\n",
    "        \"11ed59ca-df2c-2188-acbd-a95f70cb44b0\": \"Ulmus\",\n",
    "        \"11ed602d-6874-225c-acbd-a95f70cb44b0\": \"Ulmus\",\n",
    "        \"11ed6048-9fb9-4b28-acbd-a95f70cb44b0\": \"Ulmus\",\n",
    "    },\n",
    "    \n",
    "    \"other\": {\n",
    "        \"11ed65b6-6a22-1968-b56b-ae7b87f820b4\": \"Artemisia\",\n",
    "        \"11ed463e-f09f-6456-b550-ae7b87f820b4\": \"Cedrus\",\n",
    "        \"11ec6179-fde0-042c-adac-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ec617a-fb8b-e3f2-80fb-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ec5821-b371-c3dc-8359-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ec5832-4fee-03b4-8561-ae7b87f820b4\": \"Iberulites\",\n",
    "        \"11ebe542-f782-c172-bf10-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebeabd-e224-d5c4-8b63-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebedec-0da5-47ac-8066-ae7b87f820b4\": \"Waterdroplets\",\n",
    "        \"11ebee15-1fea-4c68-9cd6-ae7b87f820b4\": \"Waterdroplets\",\n",
    "    },\n",
    "    \n",
    "    \"spores\": {\n",
    "        \"11ebf9db-f2e9-98cc-bc67-ae7b87f820b4\": \"Alternaria Solani\",\n",
    "        \"11ec01b9-d571-ea8e-b7e1-ae7b87f820b4\": \"Fusarium Graminearum\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d844c1-6637-4471-a841-e44a04568a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.306749Z",
     "iopub.status.busy": "2022-12-01T14:59:17.306391Z",
     "iopub.status.idle": "2022-12-01T14:59:17.363022Z",
     "shell.execute_reply": "2022-12-01T14:59:17.361355Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.306719Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'my_first_model'\n",
    "model_path = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa66bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.366766Z",
     "iopub.status.busy": "2022-12-01T14:59:17.366248Z",
     "iopub.status.idle": "2022-12-01T14:59:17.435229Z",
     "shell.execute_reply": "2022-12-01T14:59:17.434072Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.366726Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8 # Can typically be left as is. Depending on your hardware you can try to increase the value to get better performance.\n",
    "epochs = 256\n",
    "model_features = [\n",
    "     'rec0', 'rec1'  \n",
    "] #Can typically be left as is. This defines on what data we want to train. Usually, we train on the two images 'rec0' and 'rec1'\n",
    "data_filters = [\n",
    "    #'blur', # remove blurry events\n",
    "    #'crop', # remove cropped events\n",
    "]\n",
    "data_maps = [ # /!\\ not fully functionnal yet !\n",
    "    #'process_waves', # remove \"waves\" from all events\n",
    "    #'holo_aug', # image augmentation\n",
    "]\n",
    "caching = True\n",
    "\n",
    "collections_train = [\n",
    "    #\"raw-pollens\",\n",
    "    #\"old-pollens\",\n",
    "    \"new-pollens\",\n",
    "    \"other\",\n",
    "    \"spores\",\n",
    "]\n",
    "\n",
    "# These two values only apply when collections_val is empty\n",
    "train_part = 0.7\n",
    "test_part = 0.3\n",
    "\n",
    "# Leave empty if you'd like to train and eval on the same collections\n",
    "collections_val = []\n",
    "\n",
    "classes = set(cls \n",
    "              for collection in collections_train + collections_val \n",
    "              for cls in DATASET_DEFINITIONS[collection].values())\n",
    "\n",
    "# Add important fields to the report\n",
    "report_data[\"batch_size\"] = batch_size\n",
    "report_data[\"validation_type\"] = \"train split\" if len(collections_val) == 0 else \"separate dataset\"\n",
    "report_data[\"collections_train\"] = collections_train\n",
    "report_data[\"collections_val\"] = collections_val\n",
    "report_data[\"train_part\"] = train_part\n",
    "report_data[\"test_part\"] = test_part\n",
    "report_data[\"classes\"] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de1ee9b7-b414-4d0e-a210-60c876339357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.436941Z",
     "iopub.status.busy": "2022-12-01T14:59:17.436584Z",
     "iopub.status.idle": "2022-12-01T14:59:17.505902Z",
     "shell.execute_reply": "2022-12-01T14:59:17.504690Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.436907Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(model_path, model_name, \"training\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(model_path, model_name, \"model\"), exist_ok=True)\n",
    "ds_train_cache_path = os.path.join(model_path, model_name, 'training', f'train_cache_{\"_\".join(collections_train)}{\"_\" + \"_\".join(data_filters + data_maps) if len(data_filters + data_maps) > 0 else \"\"}')\n",
    "if len(collections_val) == 0:\n",
    "    cache_name = f'test_cache_{\"_\".join(collections_train)}'\n",
    "else:\n",
    "    cache_name = f'test_cache_{\"_\".join(collections_val)}'\n",
    "ds_val_cache_path = os.path.join(model_path, model_name, 'training', f'{cache_name}{\"_\" + \"_\".join(data_filters + data_maps) if len(data_filters + data_maps) > 0 else \"\"}')\n",
    "model_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_file_path = os.path.join(model_path, model_name, 'training', 'checkpoints', model_timestamp)\n",
    "logdir = os.path.join(model_path, model_name, 'training', 'logs', model_timestamp)\n",
    "os.makedirs(logdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d074e54-8821-4cd9-99cc-32e73df216f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Computed Values\n",
    "Here we define the class weights for unbalanced datasets, the class counts, the class labels and we compute the model name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693dd81-ded3-469e-ac9f-245e5f8525b9",
   "metadata": {},
   "source": [
    "First, we define some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24e7b74-6ca9-437d-bbdb-8b26ca07d9a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.507662Z",
     "iopub.status.busy": "2022-12-01T14:59:17.507314Z",
     "iopub.status.idle": "2022-12-01T14:59:17.569951Z",
     "shell.execute_reply": "2022-12-01T14:59:17.568676Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.507629Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def get_dataset_mapping(ds_map: dict, collections: List[str], classes: List[str]):\n",
    "    \"\"\"This method filters a dataset definition for specific systems and class labels\n",
    "    and returns a flat dictionary with { <dataset-id>: <class-label> } \"\"\"\n",
    "    \n",
    "    ret = {}\n",
    "    for c in collections:\n",
    "        ret.update(\n",
    "            { key: value for key, value in ds_map[c].items() if value in classes}\n",
    "        )\n",
    "    return ret\n",
    "\n",
    "def get_dataset_sizes(ds_map_flat: dict):\n",
    "    \"\"\"Return a dict with <dataset-id>: <class-size>\"\"\"\n",
    "    \n",
    "    dataset_sizes = {}\n",
    "    for k, v in dataset_map.items():\n",
    "        result = query_interface_ml.session.query(\n",
    "            func.count(dem.EventsInEventDataset.event_id)\n",
    "        ).filter(\n",
    "            dem.EventsInEventDataset.dataset_id==uuid.UUID(k).bytes\n",
    "        ).all()\n",
    "        dataset_sizes[k] = result[0][0]\n",
    "    return dataset_sizes\n",
    "    \n",
    "def get_class_sizes(ds_map_flat: dict, dataset_sizes: dict):\n",
    "    \"\"\"Return a dict with <class-name>: <class-size>\"\"\"\n",
    "    \n",
    "    class_sizes = {}\n",
    "    for k, v in dataset_map.items():\n",
    "        size = dataset_sizes[k]\n",
    "        if v not in class_sizes: class_sizes[v] = 0\n",
    "        class_sizes[v] += size\n",
    "    return class_sizes\n",
    "\n",
    "def get_sorted_class_list(ds_map_flat: dict):\n",
    "    return sorted(list(set(ds_map_flat.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86d5aa-7390-4f1e-9efc-0a215708a2b7",
   "metadata": {},
   "source": [
    "... and then we can compute the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "599bc8ca-b765-4cd7-a647-76fe212866ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.571435Z",
     "iopub.status.busy": "2022-12-01T14:59:17.571145Z",
     "iopub.status.idle": "2022-12-01T14:59:17.778023Z",
     "shell.execute_reply": "2022-12-01T14:59:17.776876Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.571406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the dataset collection you need for the training\n",
    "dataset_map = get_dataset_mapping(\n",
    "    DATASET_DEFINITIONS,\n",
    "    collections=collections_train,\n",
    "    classes=classes\n",
    ")\n",
    "assert len(dataset_map) > 0\n",
    "\n",
    "dataset_sizes = get_dataset_sizes(dataset_map)\n",
    "class_sizes = get_class_sizes(dataset_map, dataset_sizes)\n",
    "classes = get_sorted_class_list(dataset_map)\n",
    "num_classes = len(classes)\n",
    "\n",
    "n_samples = sum(dataset_sizes.values())\n",
    "class_counts = [class_sizes[d] for d in classes]\n",
    "class_weights = n_samples / np.array(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd092e8-80f5-4e86-aef2-d62a8fb8dc89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup the dataset pipeline <a class=\"anchor\" id=\"pipeline\"></a>\n",
    "This step builds the dataset pipeline used later to train the model. Here we have to make the decision if we would like to validate on a part of the training dataset, or if we would like to define a seperate set of datasets for validation.\n",
    "\n",
    "If we use the same datasets for training as for validation, we have to define in what portions we would like to split the data into. Typically, we use 80% to 20% split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01de7a-758e-4c86-acc3-1ab74d3e4fbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Load train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17c78e34-d206-46fe-83d6-04e3443ed6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T14:59:17.779790Z",
     "iopub.status.busy": "2022-12-01T14:59:17.779478Z",
     "iopub.status.idle": "2022-12-01T15:00:54.129521Z",
     "shell.execute_reply": "2022-12-01T15:00:54.128223Z",
     "shell.execute_reply.started": "2022-12-01T14:59:17.779757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(collections_val) == 0:\n",
    "\n",
    "    dataset_train, dataset_val = query_interface_ml.prepare_tf_dataset_from_poleno_datasets(\n",
    "        dataset_list=list(dataset_map.keys()),\n",
    "        batch_size=batch_size,\n",
    "        model_features=model_features,\n",
    "        labels=classes,\n",
    "        dataset_label_mapping=dataset_map,\n",
    "        split=(train_part, test_part)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    dataset_map_val = get_dataset_mapping(\n",
    "        DATASET_DEFINITIONS,\n",
    "        collections=collections_val,\n",
    "        classes=classes\n",
    "    )\n",
    "    \n",
    "    dataset_train = query_interface_ml.prepare_tf_dataset_from_poleno_datasets(\n",
    "        dataset_list=list(dataset_map.keys()),\n",
    "        batch_size=batch_size,\n",
    "        model_features=model_features,\n",
    "        labels=classes,\n",
    "        dataset_label_mapping=dataset_map,\n",
    "    )\n",
    "    \n",
    "    dataset_val = query_interface_ml.prepare_tf_dataset_from_poleno_datasets(\n",
    "        dataset_list=list(dataset_map_val.keys()),\n",
    "        batch_size=batch_size,\n",
    "        model_features=model_features,\n",
    "        labels=classes,\n",
    "        dataset_label_mapping=dataset_map_val,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f649c94-4c3c-46bf-a48f-a3fe81fdb695",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define then apply filters and maps (e.g. data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f733876-810b-4c5d-a0e8-f1d750c5523c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:00:54.130758Z",
     "iopub.status.busy": "2022-12-01T15:00:54.130550Z",
     "iopub.status.idle": "2022-12-01T15:00:54.187392Z",
     "shell.execute_reply": "2022-12-01T15:00:54.186464Z",
     "shell.execute_reply.started": "2022-12-01T15:00:54.130738Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define data filters\n",
    "# filters out images where particles are cropped\n",
    "def filter_crop(rec0: tf.Tensor, rec1: tf.Tensor, T: float = .0001, BT: float = .85):\n",
    "    border = [0,rec0.shape[0]-1]\n",
    "    mask = np.array([\n",
    "        [1. if i in border or j in border else 0. for j in range(rec0.shape[1])]\n",
    "        for i in range(rec0.shape[0])\n",
    "    ]).reshape(*rec0.shape)\n",
    "    apply_filter_crop_ = lambda x: ((x.numpy()<BT)*mask).sum() / mask.sum() > T # return True if particle is cropped\n",
    "    return not apply_filter_crop_(rec0) and not apply_filter_crop_(rec1)\n",
    "apply_filter_crop = lambda ids, features, targets: tf.py_function( #py_function to work in eager mode (dataset operations are in graph mode by default)\n",
    "    filter_crop, [features['rec0'], features['rec1']], Tout=tf.bool\n",
    ")\n",
    "\n",
    "# filters out images where particles are blurry\n",
    "import cv2\n",
    "def filter_blur(rec0: tf.Tensor, rec1: tf.Tensor, T: float = .0014):\n",
    "    apply_filter_blur_ = lambda x: cv2.Laplacian(x.numpy(), cv2.CV_32F).var() < T # return True if image is blurred\n",
    "    return not apply_filter_blur_(rec0) and not apply_filter_blur_(rec1)\n",
    "apply_filter_blur = lambda ids, features, targets: tf.py_function( #py_function to work in eager mode (dataset operations are in graph mode by default)\n",
    "    filter_blur, [features['rec0'], features['rec1']], Tout=tf.bool\n",
    ")\n",
    "\n",
    "def filter_test(rec0: tf.Tensor, rec1: tf.Tensor):\n",
    "    apply_filter_test_ = lambda x: True\n",
    "    return not apply_filter_test_(rec0) and not apply_filter_test_(rec1)\n",
    "apply_filter_test = lambda ids, features, targets: tf.py_function( #py_function to work in eager mode (dataset operations are in graph mode by default)\n",
    "    filter_test, [features['rec0'], features['rec1']], Tout=tf.bool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435b97ab-6010-4b06-8cbe-a134436069bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:00:54.188522Z",
     "iopub.status.busy": "2022-12-01T15:00:54.188320Z",
     "iopub.status.idle": "2022-12-01T15:00:54.264945Z",
     "shell.execute_reply": "2022-12-01T15:00:54.263461Z",
     "shell.execute_reply.started": "2022-12-01T15:00:54.188503Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define data maps\n",
    "import cv2\n",
    "# removes \"waves\" around the particles\n",
    "def rmv_waves(rec: tf.Tensor):\n",
    "    img = (rec.numpy()*255).astype(np.uint8)\n",
    "    img = img.reshape(*img.shape[:-1])\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    _, mask = cv2.threshold(blurred, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    cleaned = img * ~(mask).astype(bool)\n",
    "    return cleaned\n",
    "\n",
    "# performs random image augmentation\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "holo_aug = Sequential([\n",
    "\tpreprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "\tpreprocessing.RandomZoom(-0.05),\n",
    "\tpreprocessing.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "841c3ed4-3568-40c7-bca6-f21277e319d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:00:54.267896Z",
     "iopub.status.busy": "2022-12-01T15:00:54.267359Z",
     "iopub.status.idle": "2022-12-01T15:00:54.329211Z",
     "shell.execute_reply": "2022-12-01T15:00:54.328061Z",
     "shell.execute_reply.started": "2022-12-01T15:00:54.267853Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_custom_holo_map(func, *args):\n",
    "    args[1]['rec0'] = tf.py_function(func, [args[1]['rec0']], Tout=[tf.float32])\n",
    "    args[1]['rec1'] = tf.py_function(func, [args[1]['rec1']], Tout=[tf.float32])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef37950a-3d1a-45dd-9164-6b9c00c8fcbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:00:54.331132Z",
     "iopub.status.busy": "2022-12-01T15:00:54.330816Z",
     "iopub.status.idle": "2022-12-01T15:00:54.409767Z",
     "shell.execute_reply": "2022-12-01T15:00:54.408748Z",
     "shell.execute_reply.started": "2022-12-01T15:00:54.331102Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply data filters and data maps\n",
    "dataset_train.tf_dataset = dataset_train.tf_dataset.unbatch()\n",
    "dataset_val.tf_dataset = dataset_val.tf_dataset.unbatch()\n",
    "\n",
    "if 'blur' in data_filters:\n",
    "    dataset_train.tf_dataset = dataset_train.tf_dataset.filter(apply_filter_blur)\n",
    "    dataset_val.tf_dataset = dataset_val.tf_dataset.filter(apply_filter_blur)\n",
    "if 'crop' in data_filters:\n",
    "    dataset_train.tf_dataset = dataset_train.tf_dataset.filter(apply_filter_crop)\n",
    "    dataset_val.tf_dataset = dataset_val.tf_dataset.filter(apply_filter_crop)\n",
    "if 'process_waves' in data_maps:\n",
    "    dataset_train.tf_dataset = dataset_train.tf_dataset.map(lambda *args: apply_custom_holo_map(rmv_waves, *args), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    dataset_val.tf_dataset = dataset_val.tf_dataset.map(lambda *args: apply_custom_holo_map(rmv_waves, *args), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "if 'holo_aug' in data_maps:\n",
    "    augmented_train = dataset_train.tf_dataset.map(lambda *args: apply_custom_holo_map(holo_aug, *args), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    dataset_train.tf_dataset = augmented_train.concatenate(dataset_train.tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6babf2-bc38-4f81-aec7-ee91c98008e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cache, shuffle, batch, and prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a53246-2f33-4e79-bcb5-50d4c394d73e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:00:54.411336Z",
     "iopub.status.busy": "2022-12-01T15:00:54.411024Z",
     "iopub.status.idle": "2022-12-01T15:03:16.399370Z",
     "shell.execute_reply": "2022-12-01T15:03:16.398161Z",
     "shell.execute_reply.started": "2022-12-01T15:00:54.411306Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION: Rememeber to remove the cache file if you make changes to the dataset! Otherwies, the changes will not be reflected into the dataset and the trainingwill run on the old data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache:  94%|█████████▎| 137160/146672 [02:21<00:09, 966.40it/s] \n"
     ]
    }
   ],
   "source": [
    "# Cache the pipeline to a file in order to make subsequent training passes much faster\n",
    "if caching:\n",
    "    # Cache the pipeline to a file in order to make subsequent training passes much faster\n",
    "    dataset_train.enable_cache(ds_train_cache_path, prepare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1287110-dcec-4fec-9b19-a656338f472e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:03:16.401490Z",
     "iopub.status.busy": "2022-12-01T15:03:16.400845Z",
     "iopub.status.idle": "2022-12-01T15:04:08.108872Z",
     "shell.execute_reply": "2022-12-01T15:04:08.107256Z",
     "shell.execute_reply.started": "2022-12-01T15:03:16.401451Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION: Rememeber to remove the cache file if you make changes to the dataset! Otherwies, the changes will not be reflected into the dataset and the trainingwill run on the old data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache:  94%|█████████▎| 58807/62864 [00:51<00:03, 1138.88it/s]\n"
     ]
    }
   ],
   "source": [
    "if caching:\n",
    "    dataset_val.enable_cache(ds_val_cache_path, prepare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c84866b-e9b6-4ae1-8ffc-a3b8bd6e699d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:04:08.111420Z",
     "iopub.status.busy": "2022-12-01T15:04:08.110954Z",
     "iopub.status.idle": "2022-12-01T15:06:15.313657Z",
     "shell.execute_reply": "2022-12-01T15:06:15.311898Z",
     "shell.execute_reply.started": "2022-12-01T15:04:08.111377Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'137160 train, 58807 validation'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%i train, %i validation' % (len(list(dataset_train.tf_dataset.as_numpy_iterator())), len(list(dataset_val.tf_dataset.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41bd43a7-b369-4466-9660-94a082c03a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:15.316049Z",
     "iopub.status.busy": "2022-12-01T15:06:15.315653Z",
     "iopub.status.idle": "2022-12-01T15:06:15.389441Z",
     "shell.execute_reply": "2022-12-01T15:06:15.388062Z",
     "shell.execute_reply.started": "2022-12-01T15:06:15.316010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train.tf_dataset = dataset_train.tf_dataset.shuffle(batch_size*100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "dataset_val.tf_dataset = dataset_val.tf_dataset.shuffle(batch_size*100, reshuffle_each_iteration=False).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84f9c892-3a63-4c8a-aa67-156c997fb9ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:15.396544Z",
     "iopub.status.busy": "2022-12-01T15:06:15.396177Z",
     "iopub.status.idle": "2022-12-01T15:06:15.439472Z",
     "shell.execute_reply": "2022-12-01T15:06:15.438581Z",
     "shell.execute_reply.started": "2022-12-01T15:06:15.396514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset lengths: (train: 146672, val: 62864)\n",
      "number of classes: (train: 20, val: 20)\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset lengths: (train: {dataset_train.dataset_length}, val: {dataset_val.dataset_length})\")\n",
    "print(f\"number of classes: (train: {dataset_train.num_classes}, val: {dataset_val.num_classes})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f7a1d-3969-4a0b-b1a1-0138599b9fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build the model <a class=\"anchor\" id=\"model\"></a>\n",
    "Now we can build the model we want to train. You can use any model you can imagine as long as the model has:\n",
    "- two inputs with the shape 200x200x1 with the names 'rec0' and 'rec1'\n",
    "- an output layer with the name 'target' and a dimension of #classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651abab8-ad5c-4f5a-bc37-25820cc06450",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pre-trained EffNet\n",
    "This uses transferlearning in order to get fast results. You can choose between 8 models B0 - B7, each a little larger than the previous. The way this is set up is that we remove the final layer of the pretrained network and add our own output layer corresponding to our classes.\n",
    "\n",
    "Note that in our testing, B0 is sufficient for most applications. With larger models you increase the tendency to overfit the data. If you have many classes and large datasets, you can consider increasing the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6a702e9-a3e7-41d0-a15a-61918bd21c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:15.442516Z",
     "iopub.status.busy": "2022-12-01T15:06:15.442189Z",
     "iopub.status.idle": "2022-12-01T15:06:15.484132Z",
     "shell.execute_reply": "2022-12-01T15:06:15.482389Z",
     "shell.execute_reply.started": "2022-12-01T15:06:15.442467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WARNING: THIS IS NOT A GOOD IDEA TO DO IF YOU DOWNLOAD DATA FROM UNKNOW OR UNVERIFIED SOURCES !!!\n",
    "# THIS COULD INDUCE HUGE SECURITY RISKS.\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "049435ae-c720-4cc8-b642-e4db32df4f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:15.486301Z",
     "iopub.status.busy": "2022-12-01T15:06:15.486004Z",
     "iopub.status.idle": "2022-12-01T15:06:20.841853Z",
     "shell.execute_reply": "2022-12-01T15:06:20.840392Z",
     "shell.execute_reply.started": "2022-12-01T15:06:15.486272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = []\n",
    "paths = []\n",
    "\n",
    "if \"rec0\" in model_features and \"rec1\" in model_features:\n",
    "    input0 = keras.layers.Input(shape=[200,200,1], name=\"rec0\")\n",
    "    inputs.append(input0)\n",
    "    input1 = keras.layers.Input(shape=[200,200,1], name=\"rec1\")\n",
    "    inputs.append(input1)\n",
    "\n",
    "    input0_reshape = keras.layers.Concatenate()([input0,input0,input0])\n",
    "    input1_reshape = keras.layers.Concatenate()([input1,input1,input1])\n",
    "    input0_reshape = input0_reshape * 255 # effnet expects [0, 255] data range\n",
    "    input1_reshape = input1_reshape * 255 # effnet expects [0, 255] data range\n",
    "\n",
    "    ###                                   -> VVVV <- Change here to B0 - B7 if needed.\n",
    "    effnetB0 = keras.applications.EfficientNetB0(input_shape=(200,200,3), \n",
    "                                                 include_top=False) # , weights='imagenet', drop_connect_rate=0.4\n",
    "    effnetB0.trainable = False\n",
    "\n",
    "    path0 = effnetB0(input0_reshape)\n",
    "    path1 = effnetB0(input1_reshape)\n",
    "\n",
    "    holo_path_ = keras.layers.Concatenate()([path0, path1])\n",
    "    holo_path_ = keras.layers.Flatten()(holo_path_)\n",
    "    \n",
    "    paths.append(holo_path_)\n",
    "\n",
    "if \"fl_spectra\" in model_features:\n",
    "    input_fl = keras.layers.Input(shape=[13], name=\"fl_spectra\")\n",
    "    fl_path = input_fl * 255\n",
    "    fl_path = keras.layers.Dense(255)(fl_path)\n",
    "    inputs.append(input_fl)\n",
    "    paths.append(fl_path)\n",
    "\n",
    "if len(paths) > 1:\n",
    "    path_ = keras.layers.Concatenate()(paths)\n",
    "else:\n",
    "    path_ = paths[0]\n",
    "\n",
    "path_ = keras.layers.Dropout(.4)(path_)\n",
    "outputs = keras.layers.Dense(\n",
    "    num_classes,\n",
    "    activation=\"sigmoid\",\n",
    "    name=\"target\"\n",
    ")(path_)\n",
    "#outputs = keras.layers.Softmax(name=\"target\")(outputs)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[outputs])\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfe34ce8-136a-4cb7-95c9-99252c6eeddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:20.843438Z",
     "iopub.status.busy": "2022-12-01T15:06:20.843183Z",
     "iopub.status.idle": "2022-12-01T15:06:20.937834Z",
     "shell.execute_reply": "2022-12-01T15:06:20.936684Z",
     "shell.execute_reply.started": "2022-12-01T15:06:20.843411Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " rec0 (InputLayer)              [(None, 200, 200, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rec1 (InputLayer)              [(None, 200, 200, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 200, 200, 3)  0           ['rec0[0][0]',                   \n",
      "                                                                  'rec0[0][0]',                   \n",
      "                                                                  'rec0[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 200, 200, 3)  0           ['rec1[0][0]',                   \n",
      "                                                                  'rec1[0][0]',                   \n",
      "                                                                  'rec1[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 200, 200, 3)  0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 200, 200, 3)  0          ['concatenate_1[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " efficientnetb0 (Functional)    (None, 7, 7, 1280)   4049571     ['tf.math.multiply[0][0]',       \n",
      "                                                                  'tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 7, 7, 2560)   0           ['efficientnetb0[0][0]',         \n",
      "                                                                  'efficientnetb0[1][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 125440)       0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 125440)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " target (Dense)                 (None, 20)           2508820     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,558,391\n",
      "Trainable params: 2,508,820\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0409e34-5c77-44a4-b899-c9f696aa197e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile the model\n",
    "By compiling we are adding the optimizer function and the loss function which drive the actual training. Usually, this can be left as is.\n",
    "\n",
    "If you notice that the model trains very slowly -> increase learning_rate <br>\n",
    "If the loss is fluctuating or rising -> decrease learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd3269b7-ffba-40d5-8201-176537e10f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:20.939642Z",
     "iopub.status.busy": "2022-12-01T15:06:20.939334Z",
     "iopub.status.idle": "2022-12-01T15:06:21.003449Z",
     "shell.execute_reply": "2022-12-01T15:06:21.002541Z",
     "shell.execute_reply.started": "2022-12-01T15:06:20.939610Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To compile the model, we need a special loss funtion that allows for class weights. The details are not that important. Relavent to know\n",
    "# is that this allows for inbalanced data to be trained correctly. For example, if you have two times more of class 1 than in class 2, then\n",
    "# you would like the model to ignore this difference and act as if the two sets are identical in size.\n",
    "\n",
    "class WeightedSCCE(tf.keras.losses.Loss):\n",
    "    \"\"\"Custom SparseCategoricalCrossentropy loss class that supports class weights.\"\"\"\n",
    "    def __init__(self, class_weight, from_logits=False, name='weighted_scce'):\n",
    "        if class_weight is None or all(v == 1. for v in class_weight):\n",
    "            self.class_weight = None\n",
    "        else:\n",
    "            self.class_weight = tf.convert_to_tensor(class_weight,\n",
    "                dtype=tf.float32)\n",
    "        self.reduction = keras.losses.Reduction.NONE\n",
    "        self.unreduced_scce = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=from_logits, name=name,\n",
    "            reduction=self.reduction)\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        loss = self.unreduced_scce(y_true, y_pred, sample_weight)\n",
    "        if self.class_weight is not None:\n",
    "            weight_mask = tf.gather(self.class_weight, y_true)\n",
    "            loss = tf.math.multiply(loss, weight_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cf0661e-3240-4e4f-8e24-aeac32f9caef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:21.005099Z",
     "iopub.status.busy": "2022-12-01T15:06:21.004811Z",
     "iopub.status.idle": "2022-12-01T15:06:21.079482Z",
     "shell.execute_reply": "2022-12-01T15:06:21.078189Z",
     "shell.execute_reply.started": "2022-12-01T15:06:21.005069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally we compile the ml model\n",
    "learning_rate = 0.000_005\n",
    "model.compile(\n",
    "    # Optimizer, that handles the weight adjustment while training\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),  \n",
    "    # Loss function to minimize\n",
    "    loss=WeightedSCCE(class_weights),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa6896-5250-443c-8897-d31bd389484e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Setting up TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af32a826-c31a-410b-aa73-64a058ed6172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:21.082001Z",
     "iopub.status.busy": "2022-12-01T15:06:21.081286Z",
     "iopub.status.idle": "2022-12-01T15:06:21.146216Z",
     "shell.execute_reply": "2022-12-01T15:06:21.145238Z",
     "shell.execute_reply.started": "2022-12-01T15:06:21.081953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "# https://stackoverflow.com/questions/40106949/unable-to-open-tensorboard-in-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7477158-7931-474d-b817-a232409f17de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:21.148373Z",
     "iopub.status.busy": "2022-12-01T15:06:21.147916Z",
     "iopub.status.idle": "2022-12-01T15:06:26.972255Z",
     "shell.execute_reply": "2022-12-01T15:06:26.971017Z",
     "shell.execute_reply.started": "2022-12-01T15:06:21.148333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-38bac72df6c6edc2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-38bac72df6c6edc2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir} --bind_all --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4d0bd-3d54-4cc7-93e4-f18281ccfe35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train the model\n",
    "Here the actual training starts. The only thing to set here is the number of epochs the model should be trained.\n",
    "\n",
    "Note, that the first epoch will take some time, as the dataset cache is built. Afterwards, the training will be only limited by your CPU/GPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99f0ebdf-84de-4653-8d88-fbc1cdaaeff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:26.975023Z",
     "iopub.status.busy": "2022-12-01T15:06:26.974485Z",
     "iopub.status.idle": "2022-12-01T15:06:28.049494Z",
     "shell.execute_reply": "2022-12-01T15:06:28.048144Z",
     "shell.execute_reply.started": "2022-12-01T15:06:26.974981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import itertools\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81f2b7f7-0aa6-477f-8ad0-cfc20410f712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:28.051916Z",
     "iopub.status.busy": "2022-12-01T15:06:28.051391Z",
     "iopub.status.idle": "2022-12-01T15:06:28.114313Z",
     "shell.execute_reply": "2022-12-01T15:06:28.112723Z",
     "shell.execute_reply.started": "2022-12-01T15:06:28.051885Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confusion matrix plotting at each epoch in TensorBoard\n",
    "# source: https://towardsdatascience.com/exploring-confusion-matrix-evolution-on-tensorboard-e66b39f4ac12 by Surhrut Ashtikar, last visited on 28.11.2022\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "       class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    ax.set(xticks=tick_marks,\n",
    "           yticks=tick_marks,\n",
    "           xticklabels=class_names, \n",
    "           yticklabels=class_names,\n",
    "           title='Confusion matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return fig\n",
    "    \n",
    "def plot_to_image(figure):\n",
    "    \"\"\"\n",
    "    Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\n",
    "    \"\"\"\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    \n",
    "    # Use plt.savefig to save the plot to a PNG in memory.\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Use tf.image.decode_png to convert the PNG buffer\n",
    "    # to a TF image. Make sure you use 4 channels.\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Use tf.expand_dims to add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    \n",
    "    # Use the model to predict the values from the test_images.\n",
    "    val_pred_raw = model.predict(dataset_val.get_data_pipeline().prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    val_pred = np.argmax(val_pred_raw, axis=1)\n",
    "    \n",
    "    # Calculate the confusion matrix using sklearn.metrics\n",
    "    cm = sklearn.metrics.confusion_matrix(val_labels, val_pred)\n",
    "    \n",
    "    figure = plot_confusion_matrix(cm, class_names=classes)\n",
    "    cm_image = plot_to_image(figure)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cff1eeec-0a4d-4787-a5e3-1e235f529e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:28.116924Z",
     "iopub.status.busy": "2022-12-01T15:06:28.116224Z",
     "iopub.status.idle": "2022-12-01T15:06:41.776423Z",
     "shell.execute_reply": "2022-12-01T15:06:41.775280Z",
     "shell.execute_reply.started": "2022-12-01T15:06:28.116892Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7351it [00:13, 542.89it/s]\n"
     ]
    }
   ],
   "source": [
    "val_labels = []  # store true labels\n",
    "for _, label_batch in tqdm.tqdm(dataset_val.get_data_pipeline()): # iterate over the validation dataset\n",
    "    val_labels.extend(label_batch[\"target\"].numpy()) # append true labels\n",
    "#val_labels = np.concatenate([y for _, y in dataset_val.get_data_pipeline()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "213cae0f-976a-449c-a5e8-098d4490a618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T15:06:41.779434Z",
     "iopub.status.busy": "2022-12-01T15:06:41.778841Z",
     "iopub.status.idle": "2022-12-01T15:06:41.857264Z",
     "shell.execute_reply": "2022-12-01T15:06:41.855895Z",
     "shell.execute_reply.started": "2022-12-01T15:06:41.779390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init tensorflow callbacks\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n",
    "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_file_path, save_weights_only=True, save_best_only=True, monitor='val_loss', mode='min')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7cbeb-d5a5-4c83-8b01-754d6c3f0da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset_train.get_data_pipeline().prefetch(tf.data.AUTOTUNE),\n",
    "    epochs=epochs, \n",
    "    validation_data=dataset_val.get_data_pipeline().prefetch(tf.data.AUTOTUNE),\n",
    "    callbacks=[\n",
    "        early_stopping, \n",
    "        checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        cm_callback\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cd4c5-37b3-4ed7-b07a-670d495c32a2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.978375Z",
     "iopub.status.idle": "2022-12-01T15:32:34.978996Z",
     "shell.execute_reply": "2022-12-01T15:32:34.978830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(model_path, model_name, 'training', 'checkpoints'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23c7d0-875f-42e1-a5f4-3e402e8f9ce0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.980075Z",
     "iopub.status.idle": "2022-12-01T15:32:34.980427Z",
     "shell.execute_reply": "2022-12-01T15:32:34.980277Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(os.path.join(model_path, model_name, 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214eb6e-17f1-43a3-8042-f526d5782056",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Time Series Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fb494-4ac2-4b76-b33a-43517a159689",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.981351Z",
     "iopub.status.idle": "2022-12-01T15:32:34.981687Z",
     "shell.execute_reply": "2022-12-01T15:32:34.981541Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import operator as op\n",
    "import numpy as np\n",
    "from poleno_db_interface.database.filter import AndClause, OrClause, ConditionClause, DataColumn\n",
    "import poleno_db_interface.database.model.poleno_data_model as pdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109ebc1-0d0c-43e4-8e5b-bf8755bd94fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare the tf pipeline\n",
    "Similar to the training and testing, we set up a tf dataset pipeline. Here we use the more flexible function `query_interface_ml.prepare_tf_dataset_from_event_filter` which allows us to define any filter. Here, we use this to get all the event from one SwisensPoleno in a defined time range.\n",
    "\n",
    "Additionally, we filter out all events that do not comply with the minimal size and solidity conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423a7f5-ea0f-4ebf-bf03-4201889c4d6d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.982537Z",
     "iopub.status.idle": "2022-12-01T15:32:34.982873Z",
     "shell.execute_reply": "2022-12-01T15:32:34.982728Z"
    }
   },
   "outputs": [],
   "source": [
    "from poleno_db_interface.database.filter import AndClause, ConditionClause\n",
    "import operator as op\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a44827-c8ab-49fb-b98d-141591de8c5a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.983890Z",
     "iopub.status.idle": "2022-12-01T15:32:34.984226Z",
     "shell.execute_reply": "2022-12-01T15:32:34.984082Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date  = datetime.date(2022,1,1)\n",
    "end_date    = datetime.date(2022,11,30)\n",
    "device_name = \"poleno-5\"\n",
    "\n",
    "filter_ = AndClause(\n",
    "    ConditionClause(pdm.Event.timestamp, op.gt, time.mktime(start_date.timetuple())),\n",
    "    ConditionClause(pdm.Event.timestamp, op.lt, time.mktime(end_date.timetuple())),\n",
    "    ConditionClause(pdm.Event.device_id_str, op.eq, device_name),\n",
    "    ConditionClause(pdm.ImageAnalysis.particleArea, op.ge, 625, \"img0\"),\n",
    "    ConditionClause(pdm.ImageAnalysis.particleArea, op.ge, 625, \"img1\"),\n",
    "    ConditionClause(pdm.ImageAnalysis.particleSolidity, op.ge, 0.9, \"img0\"),\n",
    "    ConditionClause(pdm.ImageAnalysis.particleSolidity, op.ge, 0.9, \"img1\"),\n",
    "    ConditionClause(pdm.ImageAnalysis.ImageData_id, op.eq, 0, \"img0\"),\n",
    "    ConditionClause(pdm.ImageAnalysis.ImageData_id, op.eq, 1, \"img1\"),\n",
    ")\n",
    "\n",
    "timeseries_dataset = query_interface_ml.prepare_tf_dataset_from_event_filter(\n",
    "    filter_=filter_,\n",
    "    batch_size=batch_size,\n",
    "    model_features=model_features\n",
    ")\n",
    "timeseries_dataset.dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1f287-9164-4750-a11b-6983710e1043",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.985149Z",
     "iopub.status.idle": "2022-12-01T15:32:34.985482Z",
     "shell.execute_reply": "2022-12-01T15:32:34.985339Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = np.array(dataset_train.labels)\n",
    "d = {'event_id': [], 'pred_class': [], 'pred_certainty': []}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "for id_batch, feature_batch in tqdm(timeseries_dataset.get_data_pipeline(with_id=True), total=timeseries_dataset.dataset_length/batch_size):\n",
    "    # compute predictions\n",
    "    preds = model.predict(feature_batch)\n",
    "    # append predicted labels and certainty\n",
    "    y_pred = np.argmax(preds, axis = - 1)\n",
    "    ind = (np.array(range(min(len(labels), len(y_pred)))), y_pred)\n",
    "    certainties = preds[ind]\n",
    "    classes = labels[y_pred]\n",
    "    df = df.append(\n",
    "        {\n",
    "            'event_id': [id_.decode() for id_ in id_batch[\"id\"].numpy()],\n",
    "            'pred_class': classes,\n",
    "            'pred_certainty': certainties\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "df = df.explode([\"event_id\", \"pred_class\", \"pred_certainty\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ae7dd-c6bd-40da-9303-fdeada47dfd6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.986600Z",
     "iopub.status.idle": "2022-12-01T15:32:34.986949Z",
     "shell.execute_reply": "2022-12-01T15:32:34.986805Z"
    }
   },
   "outputs": [],
   "source": [
    "from poleno_db_interface.database.query_utils import finalize_query\n",
    "from poleno_db_interface.database.query_utils import DataColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4388a3-afac-4b53-9061-fbe5855ec63f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.987718Z",
     "iopub.status.idle": "2022-12-01T15:32:34.988059Z",
     "shell.execute_reply": "2022-12-01T15:32:34.987913Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_dir = os.path.join(model_path, model_name, 'eval')\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "df.to_csv(os.path.join(eval_dir, 'timeseries_all.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb4e0b-1b4f-4089-aff7-317052ae25b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.988904Z",
     "iopub.status.idle": "2022-12-01T15:32:34.989244Z",
     "shell.execute_reply": "2022-12-01T15:32:34.989090Z"
    }
   },
   "outputs": [],
   "source": [
    "query_interface_ml.session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00006181-98ed-409b-bfff-ffd3948ca467",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.990080Z",
     "iopub.status.idle": "2022-12-01T15:32:34.990410Z",
     "shell.execute_reply": "2022-12-01T15:32:34.990268Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_ = [\n",
    "    DataColumn(pdm.Event.event_id_uuid),\n",
    "    DataColumn(pdm.Event.timestamp),\n",
    "]\n",
    "\n",
    "df_meta = pd.DataFrame(columns=[\"event_id\", \"event_timestamp\"])\n",
    "for df_chunk in tqdm(np.array_split(df, 100)):\n",
    "    filter_ = AndClause(\n",
    "        ConditionClause(\n",
    "            pdm.Event.event_id_bin, \"IN\", [UUID(id_).bytes for id_ in df_chunk[\"event_id\"]]\n",
    "        )\n",
    "    )\n",
    "    result = query_interface_ml.session.execute(\n",
    "        finalize_query(\n",
    "            columns=columns_,\n",
    "            filter_clause=filter_\n",
    "        )\n",
    "    )\n",
    "    df_tmp = pd.DataFrame(result, columns=[\"event_id\", \"event_timestamp\"])\n",
    "    df_meta = df_meta.append(df_tmp)\n",
    "\n",
    "df_meta[\"event_timestamp\"] = df_meta[\"event_timestamp\"].apply(float)\n",
    "df_meta[\"event_timestamp\"] = pd.to_datetime(df_meta[\"event_timestamp\"], unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b878fe-1641-4f68-a911-4fa443abf187",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.991373Z",
     "iopub.status.idle": "2022-12-01T15:32:34.991708Z",
     "shell.execute_reply": "2022-12-01T15:32:34.991562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_meta, df, left_on=\"event_id\", right_on=\"event_id\")\n",
    "merged_df = merged_df.set_index(\"event_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf6bc8-7c69-4cf4-9566-f060fef4597b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.992681Z",
     "iopub.status.idle": "2022-12-01T15:32:34.993008Z",
     "shell.execute_reply": "2022-12-01T15:32:34.992866Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b17535-1b06-4712-98a4-3cbaf1992645",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.993833Z",
     "iopub.status.idle": "2022-12-01T15:32:34.994162Z",
     "shell.execute_reply": "2022-12-01T15:32:34.994021Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[merged_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa504b8d-7c99-48b2-9a20-d5d438f16fdf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.994967Z",
     "iopub.status.idle": "2022-12-01T15:32:34.995297Z",
     "shell.execute_reply": "2022-12-01T15:32:34.995153Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv(os.path.join(eval_dir, 'timeseries_merged_all.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2340e3e-aa08-4507-a886-8e8a27d8a5ec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.996081Z",
     "iopub.status.idle": "2022-12-01T15:32:34.996559Z",
     "shell.execute_reply": "2022-12-01T15:32:34.996417Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[merged_df[\"pred_certainty\"]>0.9][merged_df.pred_class==label][\"event_id\"].resample('1d').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93e117-cfcd-44f5-b53c-01497875be0e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.997427Z",
     "iopub.status.idle": "2022-12-01T15:32:34.997754Z",
     "shell.execute_reply": "2022-12-01T15:32:34.997613Z"
    }
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "for label in labels:\n",
    "    tmp_df = merged_df[(merged_df[\"pred_certainty\"]>0.80) & (merged_df.pred_class==label)][\"event_id\"].resample('1D').count().to_frame()\n",
    "    tmp_df[\"label\"] = label\n",
    "    frames.append(tmp_df)\n",
    "all_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715a4b7-ec9b-44e0-bd1a-c45e35869e64",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.998601Z",
     "iopub.status.idle": "2022-12-01T15:32:34.998933Z",
     "shell.execute_reply": "2022-12-01T15:32:34.998789Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df = all_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab814c43-2471-4de0-8f78-584973c9becf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:34.999797Z",
     "iopub.status.idle": "2022-12-01T15:32:35.000244Z",
     "shell.execute_reply": "2022-12-01T15:32:35.000089Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a59f0-09d5-42f2-91f1-2704519cf77d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:35.000948Z",
     "iopub.status.idle": "2022-12-01T15:32:35.001295Z",
     "shell.execute_reply": "2022-12-01T15:32:35.001148Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9673a0-f276-454e-aa42-700adc0970f0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-01T15:32:35.002119Z",
     "iopub.status.idle": "2022-12-01T15:32:35.002454Z",
     "shell.execute_reply": "2022-12-01T15:32:35.002305Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex.line(\n",
    "    all_df,\n",
    "    x = \"event_timestamp\",\n",
    "    y = \"event_id\",\n",
    "    color = \"label\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
