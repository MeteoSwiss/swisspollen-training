{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------- Load packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # NB: interact with the file system\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import time # NB: time access and conversions\n",
    "import json # NB: Transmit structured data interchange format\n",
    "\n",
    "import numpy as np # NB: Work with arrays \n",
    "import pandas as pd # NB: Data manipulation : pd.Dataframe\n",
    "import tensorflow as tf # NB: To create deep learning models\n",
    "#import tensorflow_datasets as tfds # NB: added from benchmark.py (sophie)\n",
    "from matplotlib import pyplot as plt # NB: added for conf_matrix\n",
    "import seaborn as sns # NB: added for conf_matrix\n",
    "\n",
    "from ImageHelper import blobFromImage, imageFromBlob # NB: pre-rpocessing images for pre-trained keras model\n",
    "from backgroundGenerator import BackgroundGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix # NB : added from bench.yp (sophie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- configure access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0],'GPU')\n",
    "\n",
    "try: \n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12000)]) \n",
    "except: \n",
    "    print(\"Invalid device or cannot modify virtual devices once initialized.\", flush=True) \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysqlSettings = {\n",
    "    \"db_url\": os.getenv('DB_URL', TODO),\n",
    "    \"db_port\": os.getenv('DB_PORT', TODO),\n",
    "    \"db_user\": os.getenv('DB_USER', TODO),\n",
    "    \"db_pw\": os.getenv('DB_PW', TODO)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global parameters (Make sure to choose a unique name for each run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName =\"14Pol_Rain_12Mparam\" # NB: modified\n",
    "chunksize =250 # How many events should be used per dataset. TF will tain on them for x epochs before going to the next chunk of data. Choose size according to your hardware (ram, gpu, gpu-memory)\n",
    "chunkPrefetch = 2 # How many chunks should be cached in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "batchsize = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "epochsPerDatasetChunk = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set this to True if you choose to also include FL to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_fluorescence = False\n",
    "n_fl_configs=26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetList = [ #NB: pluie added at the end of the list\n",
    "    '11ea8493-7107-8db4-9bf7-ae7b87f820b4',#alnus 'alnus_20200220_p5_1_benoit' 3990\n",
    "    '11ea847a-f995-790c-830f-ae7b87f820b4',#alnus 'alnus_20200218_p2_1_benoit' 4966\n",
    "    '11ea8475-957e-347c-985a-ae7b87f820b4',#alnus 'alnus_20200214_p4_1_benoit' 3474 TOTAL ALNUS=12'430\n",
    "    '11ea8897-f50e-66a2-9876-ae7b87f820b4',#betula 'betula_20200406_p2_1_benoit' 5770\n",
    "    '11ea8632-18ed-7210-985a-ae7b87f820b4',#betula 'betula_20200407_p4_2_benoit' 6533\n",
    "    '11ea8632-1eb2-2452-bc84-ae7b87f820b4',#betula 'betula_20200406_p4_1_benoit' 2173 TOTAL BETULA=14'476\n",
    "    '11ea8f77-4ee3-aef4-b330-ae7b87f820b4',#carpinus 'carpinus_20200319_p5_2_fiona' 643\n",
    "    '11ea8f6d-3e75-9fe6-b46e-ae7b87f820b4',#carpinus 'carpinus_20200319_p2_2_fiona' 664\n",
    "    '11ea8f6d-1562-211a-8192-ae7b87f820b4',#carpinus 'carpinus_20200319_p2_3_fiona' 545\n",
    "    '11ea8f6c-b78c-d076-a542-ae7b87f820b4',#carpinus 'carpinus_20200319_p4_2_fiona' 395 TOTAL CARPINUS=2'247\n",
    "    '11ea8498-b729-d4e6-bc84-ae7b87f820b4',#corylus 'corylus_20200225_p2_2_benoit' 3736\n",
    "    '11ea8498-b083-cb92-a1a5-ae7b87f820b4',#corylus 'corylus_20200225_p2_1_benoit' 500\n",
    "    '11ea8498-afa9-cec4-a877-ae7b87f820b4',#corylus 'corylus_20200225_p5_1_benoit' 3578 TOTAL CORYLUS=7'814\n",
    "    '11ea849c-df8f-d95e-897d-ae7b87f820b4',#ulmus 'ulmus_20200311_p4_2_benoit' 3289\n",
    "    '11ea849c-db7b-2170-8b0f-ae7b87f820b4',#ulmus 'ulmus_20200311_p2_2_benoit' 2392\n",
    "    '11ea849a-0e25-4018-8814-ae7b87f820b4',#ulmus 'ulmus_20200304_p5_1_benoit' 4844 TOTAL ULMUS=10'525\n",
    "    '11ea8fa9-6c12-723a-b3dd-ae7b87f820b4',#cupressus 'cupressus_20200317_p5_1_fiona' 421\n",
    "    '11ea8fa8-fafa-aeb4-ac46-ae7b87f820b4',#cupressus 'cupressus_20200317_p2_1_fiona' 2340\n",
    "    '11ea8fa8-d163-dce2-b1cb-ae7b87f820b4',#cupressus 'cupressus_20200317_p4_1_fiona' 583 TOTAL CUPRESSUS=3'344\n",
    "    '11ea8636-313b-a6e4-a69e-ae7b87f820b4',#fagus 'fagus_20200413_p4_1_benoit' 2759\n",
    "    '11ea8635-ef91-6ab2-a877-ae7b87f820b4',#fagus 'fagus_20200407_p2_1_benoit' 3410\n",
    "    '11ea8635-eb18-6ee0-9876-ae7b87f820b4',#fagus 'fagus_20200413_p5_1_benoit' 4143 TOTAL FAGUS=10'312\n",
    "    '11ea857e-7bc5-60a0-842e-ae7b87f820b4',#fraxinus 'fraxinus_20200402_p5_2_benoit' 5703\n",
    "    '11ea857b-3d52-9034-830f-ae7b87f820b4',#fraxinus 'fraxinus_20200330_p4_1_benoit' 2621\n",
    "    '11ea857b-150e-c372-bc84-ae7b87f820b4',#fraxinus 'fraxinus_20200330_p2_1_benoit' 1712 TOTAL FRAXINUS=10'036\n",
    "    '11ea8477-cede-e7dc-897d-ae7b87f820b4',#taxus 'taxus_20200218_p4_1_benoit' 4872\n",
    "    '11ea8477-b584-b690-830f-ae7b87f820b4',#taxus 'taxus_20200218_p2_1_benoit' 5593\n",
    "    '11ea8494-33a5-2e4e-bc84-ae7b87f820b4',#taxus 'taxus_20200220_p5_1_benoit' 3411 TOTAL TAXUS=13'876\n",
    "    '11ea8af3-c533-f39e-8b25-ae7b87f820b4',#pinaceae 'picea_20200423_p2_1_fiona' 1826\n",
    "    '11ea8af1-91fc-9a46-8b25-ae7b87f820b4',#pinaceae 'picea_20200423_p4_1_fiona' 2375\n",
    "    '11ea8af0-83dc-6d66-b06c-ae7b87f820b4',#pinaceae 'picea_20200423_p5_1_fiona' 1969\n",
    "    '11ea863d-acf6-0ade-985a-ae7b87f820b4',#pinaceae 'pinus_20200421_p5_1_benoit' 3403\n",
    "    '11ea863c-2449-be52-8814-ae7b87f820b4',#pinaceae 'pinus_20200421_p2_1_benoit' 8582 TOTAL PINACEAE=18'155\n",
    "    '11ea8b83-25c9-8194-90d1-ae7b87f820b4',#platanus 'platanus_20200417_p4_1_benoit' 5603\n",
    "    '11ea8881-3721-9aa8-a907-ae7b87f820b4',#platanus 'platanus_20200417_p2_1_benoit' 5544 TOTAL PLATANUS=11'147\n",
    "    '11ea9911-49c3-2faa-86f2-ae7b87f820b4',#poaceae 'dactylis_20200518_p4_1_benoit' 2172\n",
    "    '11ea990f-ee01-8334-b3dd-ae7b87f820b4',#poaceae 'gram_20200518_p2_1_benoit' 1229\n",
    "    '11ea990c-b2bc-fe96-b46e-ae7b87f820b4',#poaceae 'gram_20200518_p5_1_benoit' 1508  TOTAL POACEAE inital=4'909 \n",
    "    '11eb5fd9-961a-313e-ac56-ae7b87f820b4',# +1 cynosurus 'POCclean_cynosurus_20200520_p4_1_fiona' 5895\n",
    "    '11eb5fd9-dd36-0a20-88f3-ae7b87f820b4',# +1 cynosurus 'POCclean_cynosurus_20200520_p2_1_' 6248 \n",
    "    '11ebe542-660e-0206-80be-ae7b87f820b4',# +1 dactylis 'poaceae_dactylis_fresh_p19_2021_tri_Nina' 3110\n",
    "    '11eb5fc3-03fa-6da2-8b42-ae7b87f820b4',# +1 dactylis 'POCclean_dactylis_20200518_p4_1' 1127\n",
    "    '11ebe540-187e-9a0c-b0e2-ae7b87f820b4',# +1 trisetum 'poaceae_trisetum_fresh_p19_2021_tri_Nina' 1377 TOTAL POACEAE=22'666\n",
    "    '11ea8893-edfb-ca84-a877-ae7b87f820b4',#populus 'populus_20200327_p5_1_benoit' 657\n",
    "    '11ea84a0-e89b-43b8-a69e-ae7b87f820b4',#populus 'populus_20200327_p2_benoit' 508\n",
    "    '11ea84a0-a2f0-ab8c-a877-ae7b87f820b4',#populus 'populus_20200327_p4_benoit' 2913 TOTAL POPULUS=4'078\n",
    "    '11ea863e-1fea-0f7c-a1a5-ae7b87f820b4',#quercus 'quercus_20200421_p4_1_benoit' 3824\n",
    "    '11ea863e-1b86-8226-a1a5-ae7b87f820b4',#quercus 'quercus_20200421_p2_1_benoit' 4768\n",
    "    '11ea863d-f388-a038-a1a5-ae7b87f820b4',#quercus 'quercus_20200421_p5_1_benoit' 2519 TOTAL QUERCUS=11'111\n",
    "    '11ebe542-f782-c172-bf10-ae7b87f820b4',# +1 pluie 'P5_Payerne_Pluie_28_04' event counts 389\n",
    "    '11ebeabd-e224-d5c4-8b63-ae7b87f820b4',# +1 pluie 'P5_Payerne_Pluie_30_04_AM' event counts 2786\n",
    "    '11ebedec-0da5-47ac-8066-ae7b87f820b4',# +1 pluie 'P5_Payerne_Pluie_30_04_PM' event counts 3179\n",
    "    '11ebee15-1fea-4c68-9cd6-ae7b87f820b4',# +1 pluie 'P16_Locarno_Pluie_29_04' event counts 7691 TOTAL PLUIE = 14045\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = [ # Labels corresponding to the datasetList, NB: labels pluie added\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    3,\n",
    "    3,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    5,\n",
    "    5,\n",
    "    5,\n",
    "    6,\n",
    "    6,\n",
    "    6,\n",
    "    7,\n",
    "    7,\n",
    "    7,\n",
    "    8,\n",
    "    8,\n",
    "    8,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    10,\n",
    "    10,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    12,\n",
    "    12,\n",
    "    12,\n",
    "    13,\n",
    "    13,\n",
    "    13,\n",
    "    14,\n",
    "    14,\n",
    "    14,\n",
    "    14\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 15 # Change this if you have multiple datasets belonging to the same label NB: +1 for rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure testset:<br>\n",
    "Valid values:<br>\n",
    "fromFirstChunk:   First chunk of data is used as test set<br>\n",
    "fromDataset:      Get test data from a dataset defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetMode = \"fromFirstChunk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the datasets for testsetMode=\"fromDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetList = [\n",
    "    #\"11ea8496-d06a-3434-aa3e-ae7b87f820b4\", \n",
    "    #\"11ea8632-1eb2-2452-bc84-ae7b87f820b4\"\n",
    "    ]\n",
    "testsetLabels = [ # Labels corresponding to the testsetList\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "    14    \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"labelList:\", labelList, flush=True)\\nprint(\"datasetList:\", datasetList, flush=True)\\nprint(\"NUM_CLASSES:\", NUM_CLASSES, flush=True)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(\"labelList:\", labelList, flush=True)\n",
    "print(\"datasetList:\", datasetList, flush=True)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, flush=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders (without tailing slash '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboardLogFolder = \"/scratch/Nina/poleno-training/logs\" \n",
    "checkpointFolder = \"/scratch/Nina/poleno-training/checkpoints\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFlInput(input):\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(input)\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(path)\n",
    "    path = tf.keras.layers.MaxPool1D(2)(path)\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(path)\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(path)\n",
    "    path = tf.keras.layers.MaxPool1D(2)(path)\n",
    "    path = tf.keras.layers.Flatten()(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_compiled_model(nClasses, with_fluorescence=False, n_fl_configs=1,strategy=strategy):\n",
    "    with strategy.scope():\n",
    "\n",
    "        in_img0 = tf.keras.layers.Input((200,200,1))\n",
    "        in_img1 = tf.keras.layers.Input((200,200,1))\n",
    "    \n",
    "        # If you want to train a model including fluorescence, you need to include these inputs in your model\n",
    "        if with_fluorescence:\n",
    "            in_fl_avg = tf.keras.layers.Input((n_fl_configs*6, 1))\n",
    "            in_fl_pha = tf.keras.layers.Input((n_fl_configs*6, 1))\n",
    "            in_fl_corrMag = tf.keras.layers.Input((n_fl_configs*6, 1))\n",
    "\n",
    "        # Define your model here!\n",
    "\n",
    "        #Image Processing\n",
    "        path1 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(in_img0)\n",
    "        path1 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.1)(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.1)(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.1)(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='valid')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.1)(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.1)(path1)\n",
    "\n",
    "        #path1 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path1)\n",
    "        #path1 = tf.keras.layers.Dropout(0.3)(path1)\n",
    "        #path1 = tf.keras.layers.Dropout(0.4)(path1)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(in_img1)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.1)(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.1)(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.1)(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='valid')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.1)(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.1)(path2)\n",
    " \n",
    "        #path2 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path2)\n",
    "        #path2 = tf.keras.layers.Dropout(0.3)(path2)\n",
    "\n",
    "        path1Flat = tf.keras.layers.Flatten()(path1)\n",
    "        path2Flat = tf.keras.layers.Flatten()(path2)\n",
    "\n",
    "        # FL Processing\n",
    "        if with_fluorescence:\n",
    "            fl_avg_path = processFlInput(in_fl_avg)\n",
    "            fl_pha_path = processFlInput(in_fl_pha)\n",
    "            fl_corrMag_path = processFlInput(in_fl_corrMag)\n",
    "            path = tf.keras.layers.Concatenate()(\n",
    "                [path1Flat, path2Flat, fl_avg_path, fl_pha_path, fl_corrMag_path]\n",
    "            )\n",
    "        else:\n",
    "            path = tf.keras.layers.Concatenate()([path1Flat, path2Flat])\n",
    "\n",
    "        #Densely(fully)-connected layer\n",
    "        path = tf.keras.layers.Dense(256)(path)\n",
    "        path = tf.keras.layers.Dropout(0.2)(path)\n",
    "        path = tf.keras.layers.Dense(128)(path)\n",
    "        path = tf.keras.layers.Dropout(0.2)(path)\n",
    "        #Densely(fully)-connected layer\n",
    "        path = tf.keras.layers.Dense(nClasses)(path)\n",
    "        #Softmax activation fct\n",
    "        output = tf.keras.layers.Softmax()(path)\n",
    "    \n",
    "        # If we work with fluorescence, we need to add all the inputs to the final model\n",
    "        if with_fluorescence:\n",
    "            model = tf.keras.Model(\n",
    "                inputs=[in_img0, in_img1, in_fl_avg, in_fl_pha, in_fl_corrMag],\n",
    "                outputs=output\n",
    "            )\n",
    "        else:\n",
    "            model = tf.keras.Model(inputs=[in_img0, in_img1], outputs=output)\n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "        model.compile(optimizer=opt,\n",
    "                        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                        metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################## DO NOT CHANGE CODE AFTER THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getPrepareFunc(with_fluorescence, label):\n",
    "    def processFLColumn(x, mapping=lambda x: x):\n",
    "        x = json.loads(x)\n",
    "        result = []\n",
    "        i = 0\n",
    "        while str(i) in x:\n",
    "            result.extend(x[str(i)])\n",
    "            i += 1\n",
    "        result = [mapping(a) for a in result]\n",
    "        return result\n",
    "    def processDf(df):\n",
    "        df[\"img0\"] = df[\"img0\"].apply(imageFromBlob)\n",
    "        df[\"img0\"] = df[\"img0\"].apply(lambda x: np.array(x, dtype=np.float))\n",
    "        df[\"img0\"] = df[\"img0\"].apply(lambda x: x/(2**16-1))\n",
    "        \n",
    "        df[\"img1\"] = df[\"img1\"].apply(imageFromBlob)\n",
    "        df[\"img1\"] = df[\"img1\"].apply(lambda x: np.array(x, dtype=np.float))\n",
    "        df[\"img1\"] = df[\"img1\"].apply(lambda x: x/(2**16-1))\n",
    "        if with_fluorescence:\n",
    "            df[\"avg\"] = df[\"avg\"].apply(\n",
    "                processFLColumn,\n",
    "                mapping = lambda x: x/0.5\n",
    "            )\n",
    "            df[\"corrPha\"] = df[\"corrPha\"].apply(\n",
    "                processFLColumn,\n",
    "                mapping = lambda x: x/np.pi\n",
    "            )\n",
    "            df[\"corrMag\"] = df[\"corrMag\"].apply(\n",
    "                processFLColumn,\n",
    "                mapping = lambda x: x/0.5\n",
    "            )\n",
    "        df[\"label\"] = label\n",
    "        return df\n",
    "    return processDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasetFromItList(itList, num_classes, batchsize, first=False):\n",
    "    df = None\n",
    "    for i, it in enumerate(itList):\n",
    "        if first:\n",
    "            dfTmp : pd.DataFrame = it.getFirst()\n",
    "        else:\n",
    "            dfTmp : pd.DataFrame = next(it)\n",
    "        if df is None:\n",
    "            df = dfTmp\n",
    "        else:\n",
    "            df = df.append(dfTmp)\n",
    "    print(\"Randomizing the sample in the set\", flush=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # NB df contains 12000 events(=250[events/(chunk*dataset)]*48[datasets])\n",
    "    print(\"Building TF-Dataset\", flush=True)\n",
    "    if with_fluorescence:\n",
    "        datasetData = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                np.array(df[\"img0\"].to_list()).reshape((len(df),200,200,1)), \n",
    "                np.array(df[\"img1\"].to_list()).reshape((len(df),200,200,1)),\n",
    "                np.array(df[\"avg\"].to_list()).reshape((len(df), n_fl_configs*6, 1)),\n",
    "                np.array(df[\"corrPha\"].to_list()).reshape((len(df), n_fl_configs*6, 1)),\n",
    "                np.array(df[\"corrMag\"].to_list()).reshape((len(df), n_fl_configs*6, 1))\n",
    "            ))\n",
    "    else:\n",
    "        datasetData = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                np.array(df[\"img0\"].to_list()).reshape((len(df),200,200,1)), \n",
    "                np.array(df[\"img1\"].to_list()).reshape((len(df),200,200,1))\n",
    "            ))\n",
    "    datasetLabels = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.one_hot(df[\"label\"].values, num_classes)\n",
    "        ))\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((datasetData, datasetLabels)).batch(batchsize) #NB dataset into batch\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "itList = []\n",
    "for i, dataset in enumerate(datasetList):\n",
    "    itList.append(\n",
    "        BackgroundGenerator(\n",
    "            dataset,\n",
    "            with_fl=with_fluorescence,\n",
    "            prefetch=chunkPrefetch,\n",
    "            mysqlSettings=mysqlSettings, \n",
    "            chunksize=chunksize,\n",
    "            reserveFirst= testsetMode==\"fromFirstChunk\",\n",
    "            prepareFunc=getPrepareFunc(with_fluorescence=with_fluorescence, label=labelList[i])\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testsetMode==\"fromDataset\":\n",
    "    testItList = []\n",
    "    for i, dataset in enumerate(testsetList):\n",
    "        testItList.append(\n",
    "            BackgroundGenerator(dataset, mysqlSettings=mysqlSettings, chunksize=chunksize, prepareFunc=getPrepareFunc(with_fluorescence=with_fluorescence, label=labelList[i]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterators are built\n"
     ]
    }
   ],
   "source": [
    "print(\"Iterators are built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model is built:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200, 200, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 200, 200, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 200, 200, 64  1664        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 200, 200, 64  1664        ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 200, 200, 64  102464      ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 200, 200, 64  102464      ['conv2d_13[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 100, 100, 64  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 100, 100, 64  0          ['conv2d_14[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100, 100, 64  0           ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 100, 100, 64  0           ['max_pooling2d_5[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 100, 100, 64  36928       ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 100, 100, 64  36928       ['dropout_5[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 100, 100, 64  36928       ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 100, 100, 64  36928       ['conv2d_15[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 50, 50, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 50, 50, 64)  0           ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 50, 64)   0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 50, 50, 64)   0           ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 50, 50, 128)  73856       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 50, 50, 128)  73856       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 50, 50, 128)  147584      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 50, 50, 128)  147584      ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 50, 50, 128)  147584      ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 50, 50, 128)  147584      ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 25, 25, 128)  0          ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 25, 25, 128)  0          ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 25, 25, 128)  0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 25, 25, 128)  0           ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 25, 25, 256)  295168      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 25, 25, 256)  295168      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 25, 25, 256)  590080      ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 25, 25, 256)  590080      ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 25, 25, 256)  590080      ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 25, 25, 256)  590080      ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 12, 12, 256)  0          ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 12, 12, 256)  0          ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 12, 12, 256)  0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 12, 12, 256)  0           ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 12, 12, 256)  590080      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 12, 12, 256)  590080      ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 12, 12, 256)  590080      ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 12, 12, 256)  590080      ['conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 12, 12, 256)  590080      ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 12, 12, 256)  590080      ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 6, 6, 256)   0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (None, 6, 6, 256)   0           ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 6, 6, 256)    0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 6, 6, 256)    0           ['max_pooling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 9216)         0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 9216)         0           ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 18432)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          4718848     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          32896       ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 128)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 15)           1935        ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 15)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,338,831\n",
      "Trainable params: 12,338,831\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model...\", flush=True)\n",
    "model = get_compiled_model(NUM_CLASSES, with_fluorescence=with_fluorescence, n_fl_configs=n_fl_configs,strategy = strategy)\n",
    "print(\"Model is built:\", flush=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointFolderPath = f\"{checkpointFolder}/{modelName}\"\n",
    "#checkpointFilePath = checkpointFolderPath + \"/weights.best_acc{accuracy:.3f}-ep{epoch:01d}.hdf5\"\n",
    "logger = tf.keras.callbacks.TensorBoard(log_dir=f\"{tensorboardLogFolder}/{modelName}\")\n",
    "saver = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointFolderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Building testset\", flush=True)\n",
    "if testsetMode==\"fromFirstChunk\":\n",
    "    testset = datasetFromItList(itList=itList, batchsize=batchsize, num_classes=NUM_CLASSES, first=True)\n",
    "if testsetMode==\"fromDataset\":\n",
    "    testset = datasetFromItList(itList=testItList, batchsize=batchsize, num_classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oss, acc = model.e valuate(testset, verbose=2)<br>\n",
    "odel.load_weights(f\"{checkpointFolder}/{modelName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasetCounter = 0\n",
    "bestAccuracy = 0.00\n",
    "confusion_final = np.zeros((NUM_CLASSES,NUM_CLASSES)) # NB: added bench.py (sophie)\n",
    "#NB: RESET THE CONFUSION MATRIX EVERY 5 LOOPS\n",
    "while True:\n",
    "    \n",
    "    #NB: Reset confusion matrix to zeros every 5 loops\n",
    "    if (datasetCounter % 5 == 0):\n",
    "        confusion_final = np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "        \n",
    "    confusion_temp = np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "    print(\"Prepare next chunk for training...\", flush=True)\n",
    "    trainingSet = datasetFromItList(itList=itList, batchsize=batchsize, num_classes=NUM_CLASSES)\n",
    "    print(f\"Training model on the current TF-Dataset (nr: {datasetCounter})\", flush=True)\n",
    "    history=model.fit(trainingSet, validation_data=testset,  epochs=epochsPerDatasetChunk, verbose=1, callbacks=[logger, saver])\n",
    "    \n",
    "   # NB: retrieve accuracy for storing best weights\n",
    "    accuraciesOfThisLoop = history.history['accuracy']\n",
    "    valAccuraciesOfThisLoop = history.history['val_accuracy']  \n",
    "      \n",
    "    for i in range(len(accuraciesOfThisLoop)-1,len(accuraciesOfThisLoop)):\n",
    "        print(\"Loop for number: \", i, \"current bestAccuracy: \", bestAccuracy, \"accuraciesOfThisLoop : \", accuraciesOfThisLoop[i], \"valAccuraciesOfThisLoop: \", valAccuraciesOfThisLoop[i], \"acc - val_acc: \", (accuraciesOfThisLoop[i] - valAccuraciesOfThisLoop[i]))\n",
    "        if (bestAccuracy < valAccuraciesOfThisLoop[i]) and ((accuraciesOfThisLoop[i] - valAccuraciesOfThisLoop[i]) < 0.1):\n",
    "            bestAccuracy = '{:.3f}'.format(round(valAccuraciesOfThisLoop[i], 3))\n",
    "            epoch = accuraciesOfThisLoop.index(accuraciesOfThisLoop[i]) + 1\n",
    "            #Alternative A: Only 1 file is saved          \n",
    "            model.save_weights(checkpointFolderPath + \"/weights.best.hdf5\")\n",
    "            #Alternative B: Every new better set of weights if saved\n",
    "            #model.save_weights(checkpointFolderPath + \"/weights.best-acc\"+bestAccuracy+\"-ep\"+str(epoch)+\"-.hdf5\")\n",
    "            print(\"The weights are saved\")\n",
    "            bestAccuracy = float(bestAccuracy)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #NB: added from nina_bench_gram\n",
    "    # confusion matrix and classification report\n",
    "    print(\"Testset\")\n",
    "    Y_pred = model.predict(testset)\n",
    "    print(\"Y_pred\")\n",
    "    # NB: output dimensions of model.predict, not num_classes x 1 ??\n",
    "    print(Y_pred[0:3,0:7])\n",
    "    y_pred = np.argmax(Y_pred, axis = 1)\n",
    "    true_categories = tf.concat([y for x, y in testset], axis=0)\n",
    "    print(true_categories)\n",
    "    # binary array, indicating the position of the true label by a 1 (if we have a 1 at the 3rd place over x labels, then the true \n",
    "    # label is the third one) \n",
    "    np_testset = tfds.as_numpy(true_categories)\n",
    "    np_testset = np.argmax(np_testset,axis=1)\n",
    "    print('True_class argmax')\n",
    "    print(np_testset[0:chunksize])\n",
    "    print('Pred_class_argmax')\n",
    "    print(y_pred[0:chunksize])\n",
    "    \n",
    "    print('Confusion Matrix')\n",
    "    confusion_temp=confusion_matrix(y_pred = y_pred, y_true = np_testset)\n",
    "    print(confusion_final)\n",
    "    confusion_final=(confusion_final+confusion_temp)\n",
    "    target_names = ['alnus', 'betula', 'carpinus', 'corylus', 'ulmus', 'cupresus', 'fagus', 'fraxinus', 'taxus', 'pinaceae', 'platanus', 'poaceae', 'populus', 'quercus', 'pluie']    \n",
    "    df_conf_mat = pd.DataFrame(confusion_final, columns = target_names, index = target_names)\n",
    "\n",
    "    #NB: added from nina_bench_gram\n",
    "    # normalized confusion matrix\n",
    "    confusion_final_norm = np.around(confusion_final.astype('float') / confusion_final.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    df_conf_mat_norm = pd.DataFrame(confusion_final_norm, columns = target_names, index = target_names)\n",
    "    print(df_conf_mat_norm)\n",
    "    \n",
    "    #NB: added from nina_bench_gram    \n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(df_conf_mat_norm, annot=True,cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    datasetCounter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
